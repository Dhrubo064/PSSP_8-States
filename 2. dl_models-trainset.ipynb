{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 12962511,
          "sourceType": "datasetVersion",
          "datasetId": 8203823
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "papermill": {
      "default_parameters": {},
      "duration": 2800.02713,
      "end_time": "2025-09-04T17:55:02.136808",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-09-04T17:08:22.109678",
      "version": "2.6.0"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "a8dca40a",
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install evaluate\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-04T17:08:27.876748Z",
          "iopub.status.busy": "2025-09-04T17:08:27.876477Z",
          "iopub.status.idle": "2025-09-04T17:08:40.160872Z",
          "shell.execute_reply": "2025-09-04T17:08:40.159725Z"
        },
        "id": "a8dca40a",
        "papermill": {
          "duration": 12.291637,
          "end_time": "2025-09-04T17:08:40.162834",
          "exception": false,
          "start_time": "2025-09-04T17:08:27.871197",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CBi-GRU**"
      ],
      "metadata": {
        "id": "Mpo8JPIPNDF0"
      },
      "id": "Mpo8JPIPNDF0"
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # =========================================================\n",
        "# # 1. Load & Clean Dataset\n",
        "# # =========================================================\n",
        "# pss_data = pd.read_csv('/content/2022-08-03-ss.processed.csv')\n",
        "# pss_data_cleaned = pss_data.dropna(subset=['sst8'])\n",
        "\n",
        "# # =========================================================\n",
        "# # 2. Train / Val / Test Split\n",
        "# # =========================================================\n",
        "# label_map = {'C': 0, 'E': 1, 'H': 2, 'B': 3, 'G': 4, 'I': 5, 'T': 6, 'S': 7}\n",
        "\n",
        "# X = pss_data['seq']   # amino acid sequences\n",
        "# y = pss_data['sst8']  # secondary structure labels\n",
        "\n",
        "# train_texts, val_texts, train_labels, val_labels = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# val_texts, test_texts, val_labels, test_labels = train_test_split(val_texts, val_labels, test_size=0.5, random_state=42)\n",
        "\n",
        "# print(\"Train size:\", len(train_texts))\n",
        "# print(\"Val size:\", len(val_texts))\n",
        "# print(\"Test size:\", len(test_texts))\n",
        "\n",
        "# # =========================================================\n",
        "# # 4. Encoding Sequences & Labels\n",
        "# # =========================================================\n",
        "# # Amino acid vocabulary\n",
        "# aa_vocab = {aa: idx+1 for idx, aa in enumerate(\"ACDEFGHIKLMNPQRSTVWY\")}\n",
        "# aa_vocab[\"X\"] = len(aa_vocab) + 1  # unknown amino acid\n",
        "# pad_idx = 0\n",
        "\n",
        "# def encode_sequence(seq, max_len=128):\n",
        "#     ids = [aa_vocab.get(aa, aa_vocab[\"X\"]) for aa in seq]\n",
        "#     ids = ids[:max_len] + [pad_idx] * (max_len - len(ids))\n",
        "#     return ids\n",
        "\n",
        "# def encode_labels(labels, max_len=128):\n",
        "#     ids = [label_map.get(l, -100) for l in labels]\n",
        "#     ids = ids[:max_len] + [-100] * (max_len - len(ids))\n",
        "#     return ids\n",
        "\n",
        "# max_len = 128\n",
        "\n",
        "# train_enc = [encode_sequence(s, max_len) for s in train_texts]\n",
        "# val_enc   = [encode_sequence(s, max_len) for s in val_texts]\n",
        "# test_enc  = [encode_sequence(s, max_len) for s in test_texts]\n",
        "\n",
        "# train_lab = [encode_labels(l, max_len) for l in train_labels]\n",
        "# val_lab   = [encode_labels(l, max_len) for l in val_labels]\n",
        "# test_lab  = [encode_labels(l, max_len) for l in test_labels]\n",
        "\n",
        "# # =========================================================\n",
        "# # 5. Torch Dataset\n",
        "# # =========================================================\n",
        "# class PSSDataset(Dataset):\n",
        "#     def __init__(self, encodings, labels):\n",
        "#         self.encodings = encodings\n",
        "#         self.labels = labels\n",
        "#     def __len__(self):\n",
        "#         return len(self.encodings)\n",
        "#     def __getitem__(self, idx):\n",
        "#         return {\n",
        "#             \"input_ids\": torch.tensor(self.encodings[idx], dtype=torch.long),\n",
        "#             \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "#         }\n",
        "\n",
        "# train_dataset = PSSDataset(train_enc, train_lab)\n",
        "# val_dataset   = PSSDataset(val_enc, val_lab)\n",
        "# test_dataset  = PSSDataset(test_enc, test_lab)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "# val_loader   = DataLoader(val_dataset, batch_size=32)\n",
        "# test_loader  = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# # =========================================================\n",
        "# # 6. CBi-GRU Model\n",
        "# # =========================================================\n",
        "# class CBiGRUModel(nn.Module):\n",
        "#     def __init__(self, vocab_size, embed_dim, hidden_dim, num_labels, pad_idx=0):\n",
        "#         super(CBiGRUModel, self).__init__()\n",
        "\n",
        "#         # Convolutional Layer for feature extraction\n",
        "#         self.conv1 = nn.Conv1d(embed_dim, 128, kernel_size=3, padding=1)\n",
        "#         self.conv2 = nn.Conv1d(128, 128, kernel_size=3, padding=1)\n",
        "\n",
        "#         # Embedding Layer\n",
        "#         self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "\n",
        "#         # GRU Layer (Bidirectional)\n",
        "#         self.gru = nn.GRU(128, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
        "\n",
        "#         # Fully Connected Layer\n",
        "#         self.classifier = nn.Linear(hidden_dim * 2, num_labels)\n",
        "\n",
        "#     def forward(self, input_ids, labels=None):\n",
        "#         # Embedding the input sequence\n",
        "#         embeds = self.embedding(input_ids)   # (batch, seq_len, embed_dim)\n",
        "#         embeds = embeds.permute(0, 2, 1)  # (batch, embed_dim, seq_len)\n",
        "\n",
        "#         # Convolutional layers for feature extraction\n",
        "#         x = self.conv1(embeds)\n",
        "#         x = self.conv2(x)\n",
        "#         x = x.permute(0, 2, 1)  # (batch, seq_len, 128)\n",
        "\n",
        "#         # GRU for sequential learning\n",
        "#         gru_out, _ = self.gru(x)   # (batch, seq_len, hidden_dim*2)\n",
        "\n",
        "#         # Linear layer for classification\n",
        "#         logits = self.classifier(gru_out)  # (batch, seq_len, num_labels)\n",
        "\n",
        "#         # Compute loss if labels are provided\n",
        "#         loss = None\n",
        "#         if labels is not None:\n",
        "#             loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "#             loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "\n",
        "#         return (loss, logits) if loss is not None else logits\n",
        "\n",
        "# # =========================================================\n",
        "# # 7. Training Loop\n",
        "# # =========================================================\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# vocab_size = len(aa_vocab) + 1\n",
        "# embed_dim = 128\n",
        "# hidden_dim = 256\n",
        "# num_labels = len(label_map)\n",
        "\n",
        "# model = CBiGRUModel(vocab_size, embed_dim, hidden_dim, num_labels, pad_idx=pad_idx).to(device)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# epochs = 20\n",
        "# for epoch in range(epochs):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "#     for batch in train_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         loss, logits = model(input_ids, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "#     avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "#     # Validation\n",
        "#     model.eval()\n",
        "#     val_loss = 0\n",
        "#     with torch.no_grad():\n",
        "#         for batch in val_loader:\n",
        "#             input_ids = batch[\"input_ids\"].to(device)\n",
        "#             labels = batch[\"labels\"].to(device)\n",
        "#             loss, _ = model(input_ids, labels)\n",
        "#             val_loss += loss.item()\n",
        "#     avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lriVaSDCNEsT",
        "outputId": "975dd364-8d25-40d0-da9d-f161ddaeba7c"
      },
      "id": "lriVaSDCNEsT",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2638687586.py:11: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  pss_data = pd.read_csv('/content/2022-08-03-ss.processed.csv')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 176490\n",
            "Val size: 22061\n",
            "Test size: 22062\n",
            "Epoch 1/20 | Train Loss: 0.9132 | Val Loss: 0.8014\n",
            "Epoch 2/20 | Train Loss: 0.7668 | Val Loss: 0.7516\n",
            "Epoch 3/20 | Train Loss: 0.7250 | Val Loss: 0.7236\n",
            "Epoch 4/20 | Train Loss: 0.7039 | Val Loss: 0.7152\n",
            "Epoch 5/20 | Train Loss: 0.6914 | Val Loss: 0.7131\n",
            "Epoch 6/20 | Train Loss: 0.6824 | Val Loss: 0.6995\n",
            "Epoch 7/20 | Train Loss: 0.6754 | Val Loss: 0.6923\n",
            "Epoch 8/20 | Train Loss: 0.6692 | Val Loss: 0.6927\n",
            "Epoch 9/20 | Train Loss: 0.6643 | Val Loss: 0.6894\n",
            "Epoch 10/20 | Train Loss: 0.6602 | Val Loss: 0.6875\n",
            "Epoch 11/20 | Train Loss: 0.6558 | Val Loss: 0.6838\n",
            "Epoch 12/20 | Train Loss: 0.6532 | Val Loss: 0.6779\n",
            "Epoch 13/20 | Train Loss: 0.6513 | Val Loss: 0.6729\n",
            "Epoch 14/20 | Train Loss: 0.6480 | Val Loss: 0.6748\n",
            "Epoch 15/20 | Train Loss: 0.6470 | Val Loss: 0.6756\n",
            "Epoch 16/20 | Train Loss: 0.6456 | Val Loss: 0.6754\n",
            "Epoch 17/20 | Train Loss: 0.6464 | Val Loss: 0.6796\n",
            "Epoch 18/20 | Train Loss: 0.6465 | Val Loss: 0.6755\n",
            "Epoch 19/20 | Train Loss: 0.6465 | Val Loss: 0.6800\n",
            "Epoch 20/20 | Train Loss: 0.6504 | Val Loss: 0.6820\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# model.eval()\n",
        "# all_preds, all_labels = [], []\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch in test_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "\n",
        "#         logits = model(input_ids)\n",
        "#         preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "#         # Flatten but ignore padding (-100 in labels)\n",
        "#         for p, l in zip(preds.cpu().numpy(), labels.cpu().numpy()):\n",
        "#             for pi, li in zip(p, l):\n",
        "#                 if li != -100:\n",
        "#                     all_preds.append(pi)\n",
        "#                     all_labels.append(li)\n",
        "\n",
        "# # Classification report\n",
        "# target_names = list(label_map.keys())\n",
        "# print(classification_report(all_labels, all_preds, target_names=target_names, digits=4))\n",
        "\n",
        "# # Q8 accuracy\n",
        "# q8_accuracy = accuracy_score(all_labels, all_preds)\n",
        "# print(f\"Q8 Accuracy: {q8_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_7tpmnvNFFr",
        "outputId": "ee59a28b-27a0-4ae2-ac20-4dcc07bb3833"
      },
      "id": "U_7tpmnvNFFr",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.7181    0.7669    0.7417    646157\n",
            "           E     0.8219    0.8608    0.8409    563979\n",
            "           H     0.8498    0.9226    0.8847    682331\n",
            "           B     0.7744    0.2266    0.3506     26516\n",
            "           G     0.6756    0.4581    0.5460     72668\n",
            "           I     0.6214    0.1680    0.2645       381\n",
            "           T     0.6414    0.5681    0.6025    255238\n",
            "           S     0.5907    0.4300    0.4977    210849\n",
            "\n",
            "    accuracy                         0.7671   2458119\n",
            "   macro avg     0.7117    0.5501    0.5911   2458119\n",
            "weighted avg     0.7589    0.7671    0.7587   2458119\n",
            "\n",
            "Q8 Accuracy: 0.7671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DNSS2**"
      ],
      "metadata": {
        "id": "X-e7uNgLMsFj"
      },
      "id": "X-e7uNgLMsFj"
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # =========================================================\n",
        "# # 1. Load & Clean Dataset\n",
        "# # =========================================================\n",
        "# pss_data = pd.read_csv('/content/2022-08-03-ss.processed.csv')\n",
        "# pss_data_cleaned = pss_data.dropna(subset=['sst8'])\n",
        "\n",
        "# # =========================================================\n",
        "# # 2. Train / Val / Test Split\n",
        "# # =========================================================\n",
        "# label_map = {'C': 0, 'E': 1, 'H': 2, 'B': 3, 'G': 4, 'I': 5, 'T': 6, 'S': 7}\n",
        "\n",
        "# X = pss_data['seq']   # amino acid sequences\n",
        "# y = pss_data['sst8']  # secondary structure labels\n",
        "\n",
        "# train_texts, val_texts, train_labels, val_labels = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# val_texts, test_texts, val_labels, test_labels = train_test_split(val_texts, val_labels, test_size=0.5, random_state=42)\n",
        "\n",
        "# print(\"Train size:\", len(train_texts))\n",
        "# print(\"Val size:\", len(val_texts))\n",
        "# print(\"Test size:\", len(test_texts))\n",
        "\n",
        "# # =========================================================\n",
        "# # 4. Encoding Sequences & Labels\n",
        "# # =========================================================\n",
        "# # Amino acid vocabulary\n",
        "# aa_vocab = {aa: idx+1 for idx, aa in enumerate(\"ACDEFGHIKLMNPQRSTVWY\")}\n",
        "# aa_vocab[\"X\"] = len(aa_vocab) + 1  # unknown amino acid\n",
        "# pad_idx = 0\n",
        "\n",
        "# def encode_sequence(seq, max_len=128):\n",
        "#     ids = [aa_vocab.get(aa, aa_vocab[\"X\"]) for aa in seq]\n",
        "#     ids = ids[:max_len] + [pad_idx] * (max_len - len(ids))\n",
        "#     return ids\n",
        "\n",
        "# def encode_labels(labels, max_len=128):\n",
        "#     ids = [label_map.get(l, -100) for l in labels]\n",
        "#     ids = ids[:max_len] + [-100] * (max_len - len(ids))\n",
        "#     return ids\n",
        "\n",
        "# max_len = 128\n",
        "\n",
        "# train_enc = [encode_sequence(s, max_len) for s in train_texts]\n",
        "# val_enc   = [encode_sequence(s, max_len) for s in val_texts]\n",
        "# test_enc  = [encode_sequence(s, max_len) for s in test_texts]\n",
        "\n",
        "# train_lab = [encode_labels(l, max_len) for l in train_labels]\n",
        "# val_lab   = [encode_labels(l, max_len) for l in val_labels]\n",
        "# test_lab  = [encode_labels(l, max_len) for l in test_labels]\n",
        "\n",
        "# # =========================================================\n",
        "# # 5. Torch Dataset\n",
        "# # =========================================================\n",
        "# class PSSDataset(Dataset):\n",
        "#     def __init__(self, encodings, labels):\n",
        "#         self.encodings = encodings\n",
        "#         self.labels = labels\n",
        "#     def __len__(self):\n",
        "#         return len(self.encodings)\n",
        "#     def __getitem__(self, idx):\n",
        "#         return {\n",
        "#             \"input_ids\": torch.tensor(self.encodings[idx], dtype=torch.long),\n",
        "#             \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "#         }\n",
        "\n",
        "# train_dataset = PSSDataset(train_enc, train_lab)\n",
        "# val_dataset   = PSSDataset(val_enc, val_lab)\n",
        "# test_dataset  = PSSDataset(test_enc, test_lab)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "# val_loader   = DataLoader(val_dataset, batch_size=32)\n",
        "# test_loader  = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# # =========================================================\n",
        "# # 6. DNSS2 Model (Modified with CNN and BiLSTM)\n",
        "# # =========================================================\n",
        "# class DNSS2Model(nn.Module):\n",
        "#     def __init__(self, vocab_size, embed_dim, hidden_dim, num_labels, pad_idx=0):\n",
        "#         super(DNSS2Model, self).__init__()\n",
        "\n",
        "#         # Embedding layer\n",
        "#         self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "\n",
        "#         # CNN Layer\n",
        "#         self.conv1 = nn.Conv1d(embed_dim, 128, kernel_size=3, padding=1)  # Conv1d for sequence data\n",
        "#         self.pool = nn.MaxPool1d(2)\n",
        "\n",
        "#         # BiLSTM Layer\n",
        "#         self.lstm = nn.LSTM(128, hidden_dim, num_layers=2, bidirectional=True, batch_first=True)\n",
        "\n",
        "#         # Output layer (classifier)\n",
        "#         self.classifier = nn.Linear(hidden_dim * 2, num_labels)\n",
        "\n",
        "#     def forward(self, input_ids, labels=None):\n",
        "#         embeds = self.embedding(input_ids)   # (batch, seq_len, embed_dim)\n",
        "#         embeds = embeds.transpose(1, 2)      # (batch, embed_dim, seq_len)\n",
        "\n",
        "#         conv_out = self.conv1(embeds)        # (batch, 128, seq_len)\n",
        "#         pooled_out = self.pool(conv_out)     # (batch, 128, seq_len//2)\n",
        "\n",
        "#         lstm_out, _ = self.lstm(pooled_out.transpose(1, 2))  # (batch, seq_len//2, hidden*2)\n",
        "\n",
        "#         logits = self.classifier(lstm_out)   # (batch, seq_len//2, num_labels)\n",
        "\n",
        "#         loss = None\n",
        "#         if labels is not None:\n",
        "#             loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "#             # Adjust labels to match the sequence length after pooling\n",
        "#             labels = labels[:, :logits.size(1)].reshape(-1)\n",
        "#             loss = loss_fct(logits.view(-1, logits.size(-1)), labels)\n",
        "\n",
        "#         return (loss, logits) if loss is not None else logits\n",
        "\n",
        "# # =========================================================\n",
        "# # 7. Training Loop\n",
        "# # =========================================================\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# vocab_size = len(aa_vocab) + 1\n",
        "# embed_dim = 128\n",
        "# hidden_dim = 256\n",
        "# num_labels = len(label_map)\n",
        "\n",
        "# model = DNSS2Model(vocab_size, embed_dim, hidden_dim, num_labels, pad_idx=pad_idx).to(device)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# epochs = 10\n",
        "# for epoch in range(epochs):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "#     for batch in train_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         loss, logits = model(input_ids, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "#     avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "#     # Validation\n",
        "#     model.eval()\n",
        "#     val_loss = 0\n",
        "#     with torch.no_grad():\n",
        "#         for batch in val_loader:\n",
        "#             input_ids = batch[\"input_ids\"].to(device)\n",
        "#             labels = batch[\"labels\"].to(device)\n",
        "#             loss, _ = model(input_ids, labels)\n",
        "#             val_loss += loss.item()\n",
        "#     avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZEeo1VqMvxb",
        "outputId": "826bf933-18dd-4cda-b5c4-3fae7c85eba4"
      },
      "id": "PZEeo1VqMvxb",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1321830254.py:11: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  pss_data = pd.read_csv('/content/2022-08-03-ss.processed.csv')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 176490\n",
            "Val size: 22061\n",
            "Test size: 22062\n",
            "Epoch 1/10 | Train Loss: 1.0282 | Val Loss: 0.8467\n",
            "Epoch 2/10 | Train Loss: 0.7714 | Val Loss: 0.7419\n",
            "Epoch 3/10 | Train Loss: 0.6893 | Val Loss: 0.6963\n",
            "Epoch 4/10 | Train Loss: 0.6406 | Val Loss: 0.6670\n",
            "Epoch 5/10 | Train Loss: 0.6065 | Val Loss: 0.6452\n",
            "Epoch 6/10 | Train Loss: 0.5797 | Val Loss: 0.6284\n",
            "Epoch 7/10 | Train Loss: 0.5588 | Val Loss: 0.6149\n",
            "Epoch 8/10 | Train Loss: 0.5406 | Val Loss: 0.6050\n",
            "Epoch 9/10 | Train Loss: 0.5242 | Val Loss: 0.5957\n",
            "Epoch 10/10 | Train Loss: 0.5099 | Val Loss: 0.5881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# model.eval()\n",
        "# all_preds, all_labels = [], []\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch in test_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "\n",
        "#         logits = model(input_ids)\n",
        "#         preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "#         # Flatten but ignore padding (-100 in labels)\n",
        "#         for p, l in zip(preds.cpu().numpy(), labels.cpu().numpy()):\n",
        "#             for pi, li in zip(p, l):\n",
        "#                 if li != -100:\n",
        "#                     all_preds.append(pi)\n",
        "#                     all_labels.append(li)\n",
        "\n",
        "# # Classification report\n",
        "# target_names = list(label_map.keys())\n",
        "# print(classification_report(all_labels, all_preds, target_names=target_names, digits=4))\n",
        "\n",
        "# # Q8 accuracy\n",
        "# q8_accuracy = accuracy_score(all_labels, all_preds)\n",
        "# print(f\"Q8 Accuracy: {q8_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3uySnuoMvtT",
        "outputId": "a4030dda-720b-4a4b-ea9f-f9bb8133d986"
      },
      "id": "s3uySnuoMvtT",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.7800    0.8156    0.7974    394168\n",
            "           E     0.8386    0.8847    0.8611    294137\n",
            "           H     0.8663    0.9311    0.8976    357253\n",
            "           B     0.7664    0.3570    0.4871     14114\n",
            "           G     0.7074    0.5496    0.6186     35551\n",
            "           I     0.5123    0.2902    0.3705       286\n",
            "           T     0.6848    0.6021    0.6408    137704\n",
            "           S     0.6609    0.4922    0.5642    112256\n",
            "\n",
            "    accuracy                         0.8006   1345469\n",
            "   macro avg     0.7271    0.6153    0.6547   1345469\n",
            "weighted avg     0.7939    0.8006    0.7943   1345469\n",
            "\n",
            "Q8 Accuracy: 0.8006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**F1DCNN-SS**"
      ],
      "metadata": {
        "id": "nzjmvwurMWkD"
      },
      "id": "nzjmvwurMWkD"
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # =========================================================\n",
        "# # 1. Load & Clean Dataset\n",
        "# # =========================================================\n",
        "# pss_data = pd.read_csv('/content/2022-08-03-ss.processed.csv')\n",
        "# pss_data_cleaned = pss_data.dropna(subset=['sst8'])\n",
        "\n",
        "# # =========================================================\n",
        "# # 2. Train / Val / Test Split\n",
        "# # =========================================================\n",
        "# label_map = {'C': 0, 'E': 1, 'H': 2, 'B': 3, 'G': 4, 'I': 5, 'T': 6, 'S': 7}\n",
        "\n",
        "# X = pss_data['seq']   # amino acid sequences\n",
        "# y = pss_data['sst8']  # secondary structure labels\n",
        "\n",
        "# train_texts, val_texts, train_labels, val_labels = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# val_texts, test_texts, val_labels, test_labels = train_test_split(val_texts, val_labels, test_size=0.5, random_state=42)\n",
        "\n",
        "# print(\"Train size:\", len(train_texts))\n",
        "# print(\"Val size:\", len(val_texts))\n",
        "# print(\"Test size:\", len(test_texts))\n",
        "\n",
        "# # =========================================================\n",
        "# # 4. Encoding Sequences & Labels\n",
        "# # =========================================================\n",
        "# # Amino acid vocabulary\n",
        "# aa_vocab = {aa: idx+1 for idx, aa in enumerate(\"ACDEFGHIKLMNPQRSTVWY\")}\n",
        "# aa_vocab[\"X\"] = len(aa_vocab) + 1  # unknown amino acid\n",
        "# pad_idx = 0\n",
        "\n",
        "# def encode_sequence(seq, max_len=128):\n",
        "#     ids = [aa_vocab.get(aa, aa_vocab[\"X\"]) for aa in seq]\n",
        "#     ids = ids[:max_len] + [pad_idx] * (max_len - len(ids))\n",
        "#     return ids\n",
        "\n",
        "# def encode_labels(labels, max_len=128):\n",
        "#     ids = [label_map.get(l, -100) for l in labels]\n",
        "#     ids = ids[:max_len] + [-100] * (max_len - len(ids))\n",
        "#     return ids\n",
        "\n",
        "# max_len = 128\n",
        "\n",
        "# train_enc = [encode_sequence(s, max_len) for s in train_texts]\n",
        "# val_enc   = [encode_sequence(s, max_len) for s in val_texts]\n",
        "# test_enc  = [encode_sequence(s, max_len) for s in test_texts]\n",
        "\n",
        "# train_lab = [encode_labels(l, max_len) for l in train_labels]\n",
        "# val_lab   = [encode_labels(l, max_len) for l in val_labels]\n",
        "# test_lab  = [encode_labels(l, max_len) for l in test_labels]\n",
        "\n",
        "# # =========================================================\n",
        "# # 5. Torch Dataset\n",
        "# # =========================================================\n",
        "# class PSSDataset(Dataset):\n",
        "#     def __init__(self, encodings, labels):\n",
        "#         self.encodings = encodings\n",
        "#         self.labels = labels\n",
        "#     def __len__(self):\n",
        "#         return len(self.encodings)\n",
        "#     def __getitem__(self, idx):\n",
        "#         return {\n",
        "#             \"input_ids\": torch.tensor(self.encodings[idx], dtype=torch.long),\n",
        "#             \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "#         }\n",
        "\n",
        "# train_dataset = PSSDataset(train_enc, train_lab)\n",
        "# val_dataset   = PSSDataset(val_enc, val_lab)\n",
        "# test_dataset  = PSSDataset(test_enc, test_lab)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "# val_loader   = DataLoader(val_dataset, batch_size=32)\n",
        "# test_loader  = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# # =========================================================\n",
        "# # 6. F1DCNN-SS Model\n",
        "# # =========================================================\n",
        "# class F1DCNNSSModel(nn.Module):\n",
        "#     def __init__(self, vocab_size, embed_dim, num_filters, filter_sizes, num_labels, pad_idx=0):\n",
        "#         super(F1DCNNSSModel, self).__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "#         self.filter_sizes = filter_sizes\n",
        "#         self.num_filters = num_filters\n",
        "\n",
        "#         # CNN layers for feature extraction\n",
        "#         self.convs = nn.ModuleList([\n",
        "#             nn.Conv1d(embed_dim, num_filters, kernel_size=f, padding=(f - 1) // 2) for f in filter_sizes\n",
        "#         ])\n",
        "\n",
        "#         # Fully connected classifier\n",
        "#         self.fc = nn.Linear(num_filters * len(filter_sizes), num_labels)\n",
        "\n",
        "#     def forward(self, input_ids, labels=None):\n",
        "#         embeds = self.embedding(input_ids)   # (batch, seq_len, embed_dim)\n",
        "#         embeds = embeds.transpose(1, 2)      # (batch, embed_dim, seq_len)\n",
        "\n",
        "#         # Apply convolutional layers with padding to maintain sequence length\n",
        "#         conv_outs = [torch.relu(conv(embeds)) for conv in self.convs]  # List of (batch, num_filters, seq_len)\n",
        "\n",
        "#         # Ensure all conv_outs have the same sequence length before concatenation\n",
        "#         # This might be necessary if padding doesn't perfectly align due to different kernel sizes\n",
        "#         min_seq_len = min([out.size(2) for out in conv_outs])\n",
        "#         conv_outs_padded = [out[:, :, :min_seq_len] for out in conv_outs]\n",
        "\n",
        "\n",
        "#         # Concatenate along the channel dimension\n",
        "#         cnn_out = torch.cat(conv_outs_padded, dim=1) # (batch, num_filters * len(filter_sizes), min_seq_len)\n",
        "\n",
        "#         # Transpose for linear layer\n",
        "#         cnn_out = cnn_out.transpose(1, 2) # (batch, min_seq_len, num_filters * len(filter_sizes))\n",
        "\n",
        "#         # Apply linear layer at each position\n",
        "#         logits = self.fc(cnn_out) # (batch, min_seq_len, num_labels)\n",
        "\n",
        "#         # Loss calculation for sequence labeling\n",
        "#         loss = None\n",
        "#         if labels is not None:\n",
        "#             loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "#             # Reshape logits and labels to (total_elements, ...)\n",
        "#             # Need to also trim labels to min_seq_len\n",
        "#             loss = loss_fct(logits.view(-1, logits.size(-1)), labels[:, :min_seq_len].contiguous().view(-1))\n",
        "\n",
        "#         return (loss, logits) if loss is not None else logits\n",
        "\n",
        "# # =========================================================\n",
        "# # 7. Training Loop\n",
        "# # =========================================================\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# vocab_size = len(aa_vocab) + 1\n",
        "# embed_dim = 128\n",
        "# num_filters = 128\n",
        "# filter_sizes = [3, 4, 5]\n",
        "# num_labels = len(label_map)\n",
        "\n",
        "# model = F1DCNNSSModel(vocab_size, embed_dim, num_filters, filter_sizes, num_labels, pad_idx=pad_idx).to(device)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# epochs = 5\n",
        "# for epoch in range(epochs):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "#     for batch in train_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         loss, logits = model(input_ids, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "#     avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "#     # Validation\n",
        "#     model.eval()\n",
        "#     val_loss = 0\n",
        "#     with torch.no_grad():\n",
        "#         for batch in val_loader:\n",
        "#             input_ids = batch[\"input_ids\"].to(device)\n",
        "#             labels = batch[\"labels\"].to(device)\n",
        "#             loss, _ = model(input_ids, labels)\n",
        "#             val_loss += loss.item()\n",
        "#     avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbK4uKw-MY8M",
        "outputId": "848c9e7b-f9b9-48b0-e648-4afacda00f83"
      },
      "id": "wbK4uKw-MY8M",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-612674803.py:11: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  pss_data = pd.read_csv('/content/2022-08-03-ss.processed.csv')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 176490\n",
            "Val size: 22061\n",
            "Test size: 22062\n",
            "Epoch 1/5 | Train Loss: 1.2864 | Val Loss: 1.2620\n",
            "Epoch 2/5 | Train Loss: 1.2560 | Val Loss: 1.2494\n",
            "Epoch 3/5 | Train Loss: 1.2489 | Val Loss: 1.2477\n",
            "Epoch 4/5 | Train Loss: 1.2453 | Val Loss: 1.2422\n",
            "Epoch 5/5 | Train Loss: 1.2425 | Val Loss: 1.2366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# model.eval()\n",
        "# all_preds, all_labels = [], []\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch in test_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "\n",
        "#         logits = model(input_ids)\n",
        "#         preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "#         # Flatten but ignore padding (-100 in labels)\n",
        "#         for p, l in zip(preds.cpu().numpy(), labels.cpu().numpy()):\n",
        "#             for pi, li in zip(p, l):\n",
        "#                 if li != -100:\n",
        "#                     all_preds.append(pi)\n",
        "#                     all_labels.append(li)\n",
        "\n",
        "# # Classification report\n",
        "# target_names = list(label_map.keys())\n",
        "# print(classification_report(all_labels, all_preds, target_names=target_names, digits=4))\n",
        "\n",
        "# # Q8 accuracy\n",
        "# q8_accuracy = accuracy_score(all_labels, all_preds)\n",
        "# print(f\"Q8 Accuracy: {q8_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj0Ty9ciMYy6",
        "outputId": "07962a76-ca30-4a26-e2f7-b70a48c73751"
      },
      "id": "Fj0Ty9ciMYy6",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.5393    0.5938    0.5652    642640\n",
            "           E     0.5664    0.6257    0.5946    560805\n",
            "           H     0.5568    0.6960    0.6187    678112\n",
            "           B     0.8044    0.0628    0.1165     26378\n",
            "           G     0.6012    0.0986    0.1695     72270\n",
            "           I     0.0000    0.0000    0.0000       380\n",
            "           T     0.4455    0.3467    0.3900    253734\n",
            "           S     0.4787    0.1321    0.2071    209560\n",
            "\n",
            "    accuracy                         0.5438   2443879\n",
            "   macro avg     0.4990    0.3195    0.3327   2443879\n",
            "weighted avg     0.5401    0.5438    0.5213   2443879\n",
            "\n",
            "Q8 Accuracy: 0.5438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DeepACLSTM**"
      ],
      "metadata": {
        "id": "uky1YO8pMLcj"
      },
      "id": "uky1YO8pMLcj"
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # =========================================================\n",
        "# # 1. Load & Clean Dataset\n",
        "# # =========================================================\n",
        "# pss_data = pd.read_csv('/content/2022-08-03-ss.processed.csv')\n",
        "# pss_data_cleaned = pss_data.dropna(subset=['sst8'])\n",
        "\n",
        "# # =========================================================\n",
        "# # 2. Train / Val / Test Split\n",
        "# # =========================================================\n",
        "# label_map = {'C': 0, 'E': 1, 'H': 2, 'B': 3, 'G': 4, 'I': 5, 'T': 6, 'S': 7}\n",
        "\n",
        "\n",
        "# X = pss_data['seq']   # amino acid sequences\n",
        "# y = pss_data['sst8']  # secondary structure labels\n",
        "\n",
        "# train_texts, val_texts, train_labels, val_labels = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# val_texts, test_texts, val_labels, test_labels = train_test_split(val_texts, val_labels, test_size=0.5, random_state=42)\n",
        "\n",
        "# print(\"Train size:\", len(train_texts))\n",
        "# print(\"Val size:\", len(val_texts))\n",
        "# print(\"Test size:\", len(test_texts))\n",
        "\n",
        "# # =========================================================\n",
        "# # 4. Encoding Sequences & Labels\n",
        "# # =========================================================\n",
        "# # Amino acid vocabulary\n",
        "# aa_vocab = {aa: idx+1 for idx, aa in enumerate(\"ACDEFGHIKLMNPQRSTVWY\")}\n",
        "# aa_vocab[\"X\"] = len(aa_vocab) + 1  # unknown amino acid\n",
        "# pad_idx = 0\n",
        "\n",
        "# def encode_sequence(seq, max_len=128):\n",
        "#     ids = [aa_vocab.get(aa, aa_vocab[\"X\"]) for aa in seq]\n",
        "#     ids = ids[:max_len] + [pad_idx] * (max_len - len(ids))\n",
        "#     return ids\n",
        "\n",
        "# def encode_labels(labels, max_len=128):\n",
        "#     ids = [label_map.get(l, -100) for l in labels]\n",
        "#     ids = ids[:max_len] + [-100] * (max_len - len(ids))\n",
        "#     return ids\n",
        "\n",
        "# max_len = 128\n",
        "\n",
        "# train_enc = [encode_sequence(s, max_len) for s in train_texts]\n",
        "# val_enc   = [encode_sequence(s, max_len) for s in val_texts]\n",
        "# test_enc  = [encode_sequence(s, max_len) for s in test_texts]\n",
        "\n",
        "# train_lab = [encode_labels(l, max_len) for l in train_labels]\n",
        "# val_lab   = [encode_labels(l, max_len) for l in val_labels]\n",
        "# test_lab  = [encode_labels(l, max_len) for l in test_labels]\n",
        "\n",
        "# # =========================================================\n",
        "# # 5. Torch Dataset\n",
        "# # =========================================================\n",
        "# class PSSDataset(Dataset):\n",
        "#     def __init__(self, encodings, labels):\n",
        "#         self.encodings = encodings\n",
        "#         self.labels = labels\n",
        "#     def __len__(self):\n",
        "#         return len(self.encodings)\n",
        "#     def __getitem__(self, idx):\n",
        "#         return {\n",
        "#             \"input_ids\": torch.tensor(self.encodings[idx], dtype=torch.long),\n",
        "#             \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "#         }\n",
        "\n",
        "# train_dataset = PSSDataset(train_enc, train_lab)\n",
        "# val_dataset   = PSSDataset(val_enc, val_lab)\n",
        "# test_dataset  = PSSDataset(test_enc, test_lab)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "# val_loader   = DataLoader(val_dataset, batch_size=32)\n",
        "# test_loader  = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# # =========================================================\n",
        "# # 6. DeepACLSTM Model\n",
        "# # =========================================================\n",
        "# class DeepACLSTM(nn.Module):\n",
        "#     def __init__(self, vocab_size, embed_dim, hidden_dim, num_labels, pad_idx=0):\n",
        "#         super(DeepACLSTM, self).__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "#         self.conv1 = nn.Conv1d(embed_dim, 64, kernel_size=3, padding=1)\n",
        "#         self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
        "#         self.lstm = nn.LSTM(128, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
        "#         self.attention = nn.Linear(hidden_dim * 2, 1)  # attention layer\n",
        "#         self.classifier = nn.Linear(hidden_dim * 2, num_labels)\n",
        "\n",
        "#     def forward(self, input_ids, labels=None):\n",
        "#         embeds = self.embedding(input_ids)   # (batch, seq_len, embed_dim)\n",
        "#         x = embeds.permute(0, 2, 1)  # Change shape for convolution (batch, embed_dim, seq_len)\n",
        "#         x = self.conv1(x)           # (batch, 64, seq_len)\n",
        "#         x = self.conv2(x)           # (batch, 128, seq_len)\n",
        "#         x = x.permute(0, 2, 1)      # (batch, seq_len, 128)\n",
        "#         lstm_out, _ = self.lstm(x)  # (batch, seq_len, hidden*2)\n",
        "\n",
        "#         # Attention mechanism\n",
        "#         attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
        "#         context = torch.sum(attn_weights * lstm_out, dim=1)\n",
        "\n",
        "#         logits = self.classifier(lstm_out)    # (batch, seq_len, num_labels)\n",
        "\n",
        "#         loss = None\n",
        "#         if labels is not None:\n",
        "#             loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "#             loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "\n",
        "#         return (loss, logits) if loss is not None else logits\n",
        "\n",
        "# # =========================================================\n",
        "# # 7. Training Loop\n",
        "# # =========================================================\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# vocab_size = len(aa_vocab) + 1\n",
        "# embed_dim = 128\n",
        "# hidden_dim = 256\n",
        "# num_labels = len(label_map)\n",
        "\n",
        "# model = DeepACLSTM(vocab_size, embed_dim, hidden_dim, num_labels, pad_idx=pad_idx).to(device)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# epochs = 10\n",
        "# for epoch in range(epochs):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "#     for batch in train_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         loss, logits = model(input_ids, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "#     avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "#     # Validation\n",
        "#     model.eval()\n",
        "#     val_loss = 0\n",
        "#     with torch.no_grad():\n",
        "#         for batch in val_loader:\n",
        "#             input_ids = batch[\"input_ids\"].to(device)\n",
        "#             labels = batch[\"labels\"].to(device)\n",
        "#             loss, _ = model(input_ids, labels)\n",
        "#             val_loss += loss.item()\n",
        "#     avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvyNRYISMCkC",
        "outputId": "bf7dbc78-4cdc-4b58-ed5d-f98890997518"
      },
      "id": "tvyNRYISMCkC",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1045192263.py:11: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  pss_data = pd.read_csv('/content/2022-08-03-ss.processed.csv')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 176490\n",
            "Val size: 22061\n",
            "Test size: 22062\n",
            "Epoch 1/10 | Train Loss: 0.9027 | Val Loss: 0.7671\n",
            "Epoch 2/10 | Train Loss: 0.7226 | Val Loss: 0.7034\n",
            "Epoch 3/10 | Train Loss: 0.6694 | Val Loss: 0.6721\n",
            "Epoch 4/10 | Train Loss: 0.6395 | Val Loss: 0.6488\n",
            "Epoch 5/10 | Train Loss: 0.6196 | Val Loss: 0.6386\n",
            "Epoch 6/10 | Train Loss: 0.6047 | Val Loss: 0.6320\n",
            "Epoch 7/10 | Train Loss: 0.5939 | Val Loss: 0.6246\n",
            "Epoch 8/10 | Train Loss: 0.5849 | Val Loss: 0.6153\n",
            "Epoch 9/10 | Train Loss: 0.5766 | Val Loss: 0.6083\n",
            "Epoch 10/10 | Train Loss: 0.5695 | Val Loss: 0.6037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# model.eval()\n",
        "# all_preds, all_labels = [], []\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch in test_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "\n",
        "#         logits = model(input_ids)\n",
        "#         preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "#         # Flatten but ignore padding (-100 in labels)\n",
        "#         for p, l in zip(preds.cpu().numpy(), labels.cpu().numpy()):\n",
        "#             for pi, li in zip(p, l):\n",
        "#                 if li != -100:\n",
        "#                     all_preds.append(pi)\n",
        "#                     all_labels.append(li)\n",
        "\n",
        "# # Classification report\n",
        "# target_names = list(label_map.keys())\n",
        "# print(classification_report(all_labels, all_preds, target_names=target_names, digits=4))\n",
        "\n",
        "# # Q8 accuracy\n",
        "# q8_accuracy = accuracy_score(all_labels, all_preds)\n",
        "# print(f\"Q8 Accuracy: {q8_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyzESDE2MChU",
        "outputId": "b0d5d36c-ddec-4ad1-8616-17f04820a232"
      },
      "id": "XyzESDE2MChU",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.7554    0.7990    0.7766    646157\n",
            "           E     0.8582    0.8786    0.8683    563979\n",
            "           H     0.8706    0.9343    0.9013    682331\n",
            "           B     0.7798    0.3044    0.4379     26516\n",
            "           G     0.7159    0.5293    0.6086     72668\n",
            "           I     0.4459    0.3675    0.4029       381\n",
            "           T     0.6723    0.6290    0.6499    255238\n",
            "           S     0.6509    0.4995    0.5653    210849\n",
            "\n",
            "    accuracy                         0.7981   2458119\n",
            "   macro avg     0.7186    0.6177    0.6513   2458119\n",
            "weighted avg     0.7924    0.7981    0.7923   2458119\n",
            "\n",
            "Q8 Accuracy: 0.7981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNNH PSS**"
      ],
      "metadata": {
        "id": "WpCj5e2eJIeM"
      },
      "id": "WpCj5e2eJIeM"
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # =========================================================\n",
        "# # 1. Load & Clean Dataset\n",
        "# # =========================================================\n",
        "# pss_data = pd.read_csv('/content/2022-08-03-ss.processed.csv')\n",
        "# pss_data_cleaned = pss_data.dropna(subset=['sst8'])\n",
        "\n",
        "# # =========================================================\n",
        "# # 2. Train / Val / Test Split\n",
        "# # =========================================================\n",
        "# label_map = {'C': 0, 'E': 1, 'H': 2, 'B': 3, 'G': 4, 'I': 5, 'T': 6, 'S': 7}\n",
        "\n",
        "# X = pss_data_cleaned['seq']   # amino acid sequences\n",
        "# y = pss_data_cleaned['sst8']  # secondary structure labels\n",
        "\n",
        "# train_texts, val_texts, train_labels, val_labels = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# val_texts, test_texts, val_labels, test_labels = train_test_split(val_texts, val_labels, test_size=0.5, random_state=42)\n",
        "\n",
        "# print(\"Train size:\", len(train_texts))\n",
        "# print(\"Val size:\", len(val_texts))\n",
        "# print(\"Test size:\", len(test_texts))\n",
        "\n",
        "# # =========================================================\n",
        "# # 4. Encoding Sequences & Labels\n",
        "# # =========================================================\n",
        "# # Amino acid vocabulary\n",
        "# aa_vocab = {aa: idx+1 for idx, aa in enumerate(\"ACDEFGHIKLMNPQRSTVWY\")}\n",
        "# aa_vocab[\"X\"] = len(aa_vocab) + 1  # unknown amino acid\n",
        "# pad_idx = 0\n",
        "\n",
        "# def encode_sequence(seq, max_len=128):\n",
        "#     ids = [aa_vocab.get(aa, aa_vocab[\"X\"]) for aa in seq]\n",
        "#     ids = ids[:max_len] + [pad_idx] * (max_len - len(ids))\n",
        "#     return ids\n",
        "\n",
        "# def encode_labels(labels, max_len=128):\n",
        "#     ids = [label_map.get(l, -100) for l in labels]\n",
        "#     ids = ids[:max_len] + [-100] * (max_len - len(ids))\n",
        "#     return ids\n",
        "\n",
        "# max_len = 128\n",
        "\n",
        "# train_enc = [encode_sequence(s, max_len) for s in train_texts]\n",
        "# val_enc   = [encode_sequence(s, max_len) for s in val_texts]\n",
        "# test_enc  = [encode_sequence(s, max_len) for s in test_texts]\n",
        "\n",
        "# train_lab = [encode_labels(l, max_len) for l in train_labels]\n",
        "# val_lab   = [encode_labels(l, max_len) for l in val_labels]\n",
        "# test_lab  = [encode_labels(l, max_len) for l in test_labels]\n",
        "\n",
        "# # =========================================================\n",
        "# # 5. Torch Dataset\n",
        "# # =========================================================\n",
        "# class PSSDataset(Dataset):\n",
        "#     def __init__(self, encodings, labels):\n",
        "#         self.encodings = encodings\n",
        "#         self.labels = labels\n",
        "#     def __len__(self):\n",
        "#         return len(self.encodings)\n",
        "#     def __getitem__(self, idx):\n",
        "#         return {\n",
        "#             \"input_ids\": torch.tensor(self.encodings[idx], dtype=torch.long),\n",
        "#             \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "#         }\n",
        "\n",
        "# train_dataset = PSSDataset(train_enc, train_lab)\n",
        "# val_dataset   = PSSDataset(val_enc, val_lab)\n",
        "# test_dataset  = PSSDataset(test_enc, test_lab)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "# val_loader   = DataLoader(val_dataset, batch_size=32)\n",
        "# test_loader  = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# # =========================================================\n",
        "# # 6. CNN Model for PSSP (Adapted for Sequence Labeling)\n",
        "# # =========================================================\n",
        "# class CNNModel(nn.Module):\n",
        "#     def __init__(self, vocab_size, embed_dim, num_filters, kernel_size, num_labels, pad_idx=0):\n",
        "#         super(CNNModel, self).__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "#         # Adjusted CNN layers for sequence labeling\n",
        "#         # Use padding to maintain sequence length\n",
        "#         padding = (kernel_size - 1) // 2\n",
        "#         self.conv1 = nn.Conv1d(embed_dim, num_filters, kernel_size=kernel_size, padding=padding)\n",
        "#         # Remove pooling and flattening\n",
        "#         self.classifier = nn.Linear(num_filters, num_labels) # Classifier for each position\n",
        "\n",
        "#     def forward(self, input_ids, labels=None):\n",
        "#         embeds = self.embedding(input_ids)  # (batch, seq_len, embed_dim)\n",
        "#         embeds = embeds.permute(0, 2, 1)   # Change shape to (batch, embed_dim, seq_len)\n",
        "\n",
        "#         x = self.conv1(embeds)       # (batch, num_filters, seq_len) after padding\n",
        "\n",
        "#         x = x.transpose(1, 2) # (batch, seq_len, num_filters)\n",
        "#         logits = self.classifier(x)  # (batch, seq_len, num_labels)\n",
        "\n",
        "#         loss = None\n",
        "#         if labels is not None:\n",
        "#             loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "#             # Reshape logits and labels for CrossEntropyLoss\n",
        "#             loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "\n",
        "#         return (loss, logits) if loss is not None else logits\n",
        "\n",
        "# # =========================================================\n",
        "# # 7. Training Loop\n",
        "# # =========================================================\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# vocab_size = len(aa_vocab) + 1\n",
        "# embed_dim = 128\n",
        "# num_filters = 64\n",
        "# filter_size = 3 # Using a kernel size of 3\n",
        "# num_labels = len(label_map)\n",
        "\n",
        "# model = CNNModel(vocab_size, embed_dim, num_filters, filter_size, num_labels, pad_idx=pad_idx).to(device)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# epochs = 5\n",
        "# for epoch in range(epochs):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "#     for batch in train_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         loss, logits = model(input_ids, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "#     avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "#     # Validation\n",
        "#     model.eval()\n",
        "#     val_loss = 0\n",
        "#     with torch.no_grad():\n",
        "#         for batch in val_loader:\n",
        "#             input_ids = batch[\"input_ids\"].to(device)\n",
        "#             labels = batch[\"labels\"].to(device)\n",
        "#             loss, _ = model(input_ids, labels)\n",
        "#             val_loss += loss.item()\n",
        "#     avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBzxBzPNI3xj",
        "outputId": "c7090573-af0b-461b-84aa-a991e9e4e4ec"
      },
      "id": "TBzxBzPNI3xj",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3066850092.py:11: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  pss_data = pd.read_csv('/content/2022-08-03-ss.processed.csv')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 176490\n",
            "Val size: 22061\n",
            "Test size: 22062\n",
            "Epoch 1/5 | Train Loss: 1.4681 | Val Loss: 1.4626\n",
            "Epoch 2/5 | Train Loss: 1.4639 | Val Loss: 1.4649\n",
            "Epoch 3/5 | Train Loss: 1.4633 | Val Loss: 1.4612\n",
            "Epoch 4/5 | Train Loss: 1.4629 | Val Loss: 1.4617\n",
            "Epoch 5/5 | Train Loss: 1.4628 | Val Loss: 1.4609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# model.eval()\n",
        "# all_preds, all_labels = [], []\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch in test_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "\n",
        "#         logits = model(input_ids)\n",
        "#         preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "#         # Flatten but ignore padding (-100 in labels)\n",
        "#         for p, l in zip(preds.cpu().numpy(), labels.cpu().numpy()):\n",
        "#             for pi, li in zip(p, l):\n",
        "#                 if li != -100:\n",
        "#                     all_preds.append(pi)\n",
        "#                     all_labels.append(li)\n",
        "\n",
        "# # Classification report\n",
        "# target_names = list(label_map.keys())\n",
        "# print(classification_report(all_labels, all_preds, target_names=target_names, digits=4))\n",
        "\n",
        "# # Q8 accuracy\n",
        "# q8_accuracy = accuracy_score(all_labels, all_preds)\n",
        "# print(f\"Q8 Accuracy: {q8_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yX6s4dzI3ps",
        "outputId": "45b8e117-9b36-4c15-8ffa-2297d17084a2"
      },
      "id": "8yX6s4dzI3ps",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.4085    0.5338    0.4628    646157\n",
            "           E     0.4545    0.4384    0.4463    563979\n",
            "           H     0.4315    0.5943    0.5000    682331\n",
            "           B     0.0000    0.0000    0.0000     26516\n",
            "           G     0.0000    0.0000    0.0000     72668\n",
            "           I     0.0000    0.0000    0.0000       381\n",
            "           T     0.3403    0.1733    0.2297    255238\n",
            "           S     0.0000    0.0000    0.0000    210849\n",
            "\n",
            "    accuracy                         0.4239   2458119\n",
            "   macro avg     0.2044    0.2175    0.2049   2458119\n",
            "weighted avg     0.3668    0.4239    0.3867   2458119\n",
            "\n",
            "Q8 Accuracy: 0.4239\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**eCRRNN**"
      ],
      "metadata": {
        "id": "yJlLFzfVGimc"
      },
      "id": "yJlLFzfVGimc"
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # =========================================================\n",
        "# # 1. Load & Clean Dataset\n",
        "# # =========================================================\n",
        "# pss_data = pd.read_csv('/content/2022-08-03-ss.processed.csv')\n",
        "# pss_data_cleaned = pss_data.dropna(subset=['sst8'])\n",
        "\n",
        "# # =========================================================\n",
        "# # 2. Train / Val / Test Split\n",
        "# # =========================================================\n",
        "# label_map = {'C': 0, 'E': 1, 'H': 2, 'B': 3, 'G': 4, 'I': 5, 'T': 6, 'S': 7}\n",
        "\n",
        "# X = pss_data['seq']   # amino acid sequences\n",
        "# y = pss_data['sst8']  # secondary structure labels\n",
        "\n",
        "# train_texts, val_texts, train_labels, val_labels = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# val_texts, test_texts, val_labels, test_labels = train_test_split(val_texts, val_labels, test_size=0.5, random_state=42)\n",
        "\n",
        "# print(\"Train size:\", len(train_texts))\n",
        "# print(\"Val size:\", len(val_texts))\n",
        "# print(\"Test size:\", len(test_texts))\n",
        "\n",
        "# # =========================================================\n",
        "# # 4. Encoding Sequences & Labels\n",
        "# # =========================================================\n",
        "# # Amino acid vocabulary\n",
        "# aa_vocab = {aa: idx+1 for idx, aa in enumerate(\"ACDEFGHIKLMNPQRSTVWY\")}\n",
        "# aa_vocab[\"X\"] = len(aa_vocab) + 1  # unknown amino acid\n",
        "# pad_idx = 0\n",
        "\n",
        "# def encode_sequence(seq, max_len=128):\n",
        "#     ids = [aa_vocab.get(aa, aa_vocab[\"X\"]) for aa in seq]\n",
        "#     ids = ids[:max_len] + [pad_idx] * (max_len - len(ids))\n",
        "#     return ids\n",
        "\n",
        "# def encode_labels(labels, max_len=128):\n",
        "#     ids = [label_map.get(l, -100) for l in labels]\n",
        "#     ids = ids[:max_len] + [-100] * (max_len - len(ids))\n",
        "#     return ids\n",
        "\n",
        "# max_len = 128\n",
        "\n",
        "# train_enc = [encode_sequence(s, max_len) for s in train_texts]\n",
        "# val_enc   = [encode_sequence(s, max_len) for s in val_texts]\n",
        "# test_enc  = [encode_sequence(s, max_len) for s in test_texts]\n",
        "\n",
        "# train_lab = [encode_labels(l, max_len) for l in train_labels]\n",
        "# val_lab   = [encode_labels(l, max_len) for l in val_labels]\n",
        "# test_lab  = [encode_labels(l, max_len) for l in test_labels]\n",
        "\n",
        "# # =========================================================\n",
        "# # 5. Torch Dataset\n",
        "# # =========================================================\n",
        "# class PSSDataset(Dataset):\n",
        "#     def __init__(self, encodings, labels):\n",
        "#         self.encodings = encodings\n",
        "#         self.labels = labels\n",
        "#     def __len__(self):\n",
        "#         return len(self.encodings)\n",
        "#     def __getitem__(self, idx):\n",
        "#         return {\n",
        "#             \"input_ids\": torch.tensor(self.encodings[idx], dtype=torch.long),\n",
        "#             \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "#         }\n",
        "\n",
        "# train_dataset = PSSDataset(train_enc, train_lab)\n",
        "# val_dataset   = PSSDataset(val_enc, val_lab)\n",
        "# test_dataset  = PSSDataset(test_enc, test_lab)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "# val_loader   = DataLoader(val_dataset, batch_size=32)\n",
        "# test_loader  = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# # =========================================================\n",
        "# # 6. eCRRNN Model (Modified Architecture)\n",
        "# # =========================================================\n",
        "# class eCRRNNModel(nn.Module):\n",
        "#     def __init__(self, vocab_size, embed_dim, hidden_dim, num_labels, pad_idx=0):\n",
        "#         super(eCRRNNModel, self).__init__()\n",
        "\n",
        "#         # Embedding layer\n",
        "#         self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "\n",
        "#         # Convolutional layer (CNN part of eCRRNN)\n",
        "#         self.conv1 = nn.Conv1d(embed_dim, 256, kernel_size=5, padding=2)  # Add 1D convolution\n",
        "#         self.conv2 = nn.Conv1d(256, 512, kernel_size=5, padding=2)\n",
        "\n",
        "#         # Recurrent layer (GRU or LSTM, RNN part of eCRRNN)\n",
        "#         self.gru = nn.GRU(512, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
        "\n",
        "#         # Fully connected layer for classification\n",
        "#         self.classifier = nn.Linear(hidden_dim * 2, num_labels)\n",
        "\n",
        "#     def forward(self, input_ids, labels=None):\n",
        "#         # Embedding layer\n",
        "#         embeds = self.embedding(input_ids)   # (batch, seq_len, embed_dim)\n",
        "\n",
        "#         # Reshape for Conv1D input: (batch, embed_dim, seq_len)\n",
        "#         embeds = embeds.permute(0, 2, 1)\n",
        "\n",
        "#         # Convolutional layers\n",
        "#         conv_out = self.conv1(embeds)         # (batch, 256, seq_len)\n",
        "#         conv_out = torch.relu(conv_out)\n",
        "#         conv_out = self.conv2(conv_out)       # (batch, 512, seq_len)\n",
        "#         conv_out = torch.relu(conv_out)\n",
        "\n",
        "#         # Reshape for GRU input: (batch, seq_len, 512)\n",
        "#         conv_out = conv_out.permute(0, 2, 1)\n",
        "\n",
        "#         # Recurrent layer (GRU)\n",
        "#         gru_out, _ = self.gru(conv_out)      # (batch, seq_len, hidden*2)\n",
        "\n",
        "#         # Classification layer\n",
        "#         logits = self.classifier(gru_out)    # (batch, seq_len, num_labels)\n",
        "\n",
        "#         loss = None\n",
        "#         if labels is not None:\n",
        "#             loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "#             loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "\n",
        "#         return (loss, logits) if loss is not None else logits\n",
        "\n",
        "# # =========================================================\n",
        "# # 7. Training Loop\n",
        "# # =========================================================\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# vocab_size = len(aa_vocab) + 1\n",
        "# embed_dim = 128\n",
        "# hidden_dim = 256\n",
        "# num_labels = len(label_map)\n",
        "\n",
        "# model = eCRRNNModel(vocab_size, embed_dim, hidden_dim, num_labels, pad_idx=pad_idx).to(device)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# epochs = 10\n",
        "# for epoch in range(epochs):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "#     for batch in train_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         loss, logits = model(input_ids, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "#     avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "#     # Validation\n",
        "#     model.eval()\n",
        "#     val_loss = 0\n",
        "#     with torch.no_grad():\n",
        "#         for batch in val_loader:\n",
        "#             input_ids = batch[\"input_ids\"].to(device)\n",
        "#             labels = batch[\"labels\"].to(device)\n",
        "#             loss, _ = model(input_ids, labels)\n",
        "#             val_loss += loss.item()\n",
        "#     avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdrBiagvFZRK",
        "outputId": "ba8928be-4cd8-4e1f-edb5-530f944b9166"
      },
      "id": "rdrBiagvFZRK",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-908720724.py:11: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  pss_data = pd.read_csv('/content/2022-08-03-ss.processed.csv')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 176490\n",
            "Val size: 22061\n",
            "Test size: 22062\n",
            "Epoch 1/10 | Train Loss: 0.7933 | Val Loss: 0.6482\n",
            "Epoch 2/10 | Train Loss: 0.6041 | Val Loss: 0.5976\n",
            "Epoch 3/10 | Train Loss: 0.5605 | Val Loss: 0.5751\n",
            "Epoch 4/10 | Train Loss: 0.5369 | Val Loss: 0.5618\n",
            "Epoch 5/10 | Train Loss: 0.5185 | Val Loss: 0.5512\n",
            "Epoch 6/10 | Train Loss: 0.5027 | Val Loss: 0.5394\n",
            "Epoch 7/10 | Train Loss: 0.4873 | Val Loss: 0.5252\n",
            "Epoch 8/10 | Train Loss: 0.4721 | Val Loss: 0.5112\n",
            "Epoch 9/10 | Train Loss: 0.4580 | Val Loss: 0.5020\n",
            "Epoch 10/10 | Train Loss: 0.4453 | Val Loss: 0.4932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# model.eval()\n",
        "# all_preds, all_labels = [], []\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch in test_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "\n",
        "#         logits = model(input_ids)\n",
        "#         preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "#         # Flatten but ignore padding (-100 in labels)\n",
        "#         for p, l in zip(preds.cpu().numpy(), labels.cpu().numpy()):\n",
        "#             for pi, li in zip(p, l):\n",
        "#                 if li != -100:\n",
        "#                     all_preds.append(pi)\n",
        "#                     all_labels.append(li)\n",
        "\n",
        "# # Classification report\n",
        "# target_names = list(label_map.keys())\n",
        "# print(classification_report(all_labels, all_preds, target_names=target_names, digits=4))\n",
        "\n",
        "# # Q8 accuracy\n",
        "# q8_accuracy = accuracy_score(all_labels, all_preds)\n",
        "# print(f\"Q8 Accuracy: {q8_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvzpKbVDFZHs",
        "outputId": "5f50f04c-c143-4e82-8679-f3ab9050220c"
      },
      "id": "gvzpKbVDFZHs",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.7946    0.8430    0.8181    646157\n",
            "           E     0.8905    0.9076    0.8990    563979\n",
            "           H     0.9067    0.9407    0.9234    682331\n",
            "           B     0.7577    0.4420    0.5583     26516\n",
            "           G     0.7363    0.6469    0.6887     72668\n",
            "           I     0.7674    0.1732    0.2827       381\n",
            "           T     0.7248    0.6986    0.7115    255238\n",
            "           S     0.7201    0.5615    0.6310    210849\n",
            "\n",
            "    accuracy                         0.8356   2458119\n",
            "   macro avg     0.7873    0.6517    0.6891   2458119\n",
            "weighted avg     0.8319    0.8356    0.8320   2458119\n",
            "\n",
            "Q8 Accuracy: 0.8356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CRRNN**"
      ],
      "metadata": {
        "id": "qQWjrqVyAq1j"
      },
      "id": "qQWjrqVyAq1j"
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # =========================================================\n",
        "# # 1. Load & Clean Dataset\n",
        "# # =========================================================\n",
        "# pss_data = pd.read_csv('/content/2022-08-03-ss.processed.csv')\n",
        "# pss_data_cleaned = pss_data.dropna(subset=['sst8'])\n",
        "\n",
        "# # =========================================================\n",
        "# # 2. Train / Val / Test Split\n",
        "# # =========================================================\n",
        "# label_map = {'C': 0, 'E': 1, 'H': 2, 'B': 3, 'G': 4, 'I': 5, 'T': 6, 'S': 7}\n",
        "\n",
        "# X = pss_data['seq']   # amino acid sequences\n",
        "# y = pss_data['sst8']  # secondary structure labels\n",
        "\n",
        "# train_texts, val_texts, train_labels, val_labels = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# val_texts, test_texts, val_labels, test_labels = train_test_split(val_texts, val_labels, test_size=0.5, random_state=42)\n",
        "\n",
        "# print(\"Train size:\", len(train_texts))\n",
        "# print(\"Val size:\", len(val_texts))\n",
        "# print(\"Test size:\", len(test_texts))\n",
        "\n",
        "# # =========================================================\n",
        "# # 4. Encoding Sequences & Labels\n",
        "# # =========================================================\n",
        "# # Amino acid vocabulary\n",
        "# aa_vocab = {aa: idx+1 for idx, aa in enumerate(\"ACDEFGHIKLMNPQRSTVWY\")}\n",
        "# aa_vocab[\"X\"] = len(aa_vocab) + 1  # unknown amino acid\n",
        "# pad_idx = 0\n",
        "\n",
        "# def encode_sequence(seq, max_len=128):\n",
        "#     ids = [aa_vocab.get(aa, aa_vocab[\"X\"]) for aa in seq]\n",
        "#     ids = ids[:max_len] + [pad_idx] * (max_len - len(ids))\n",
        "#     return ids\n",
        "\n",
        "# def encode_labels(labels, max_len=128):\n",
        "#     ids = [label_map.get(l, -100) for l in labels]\n",
        "#     ids = ids[:max_len] + [-100] * (max_len - len(ids))\n",
        "#     return ids\n",
        "\n",
        "# max_len = 128\n",
        "\n",
        "# train_enc = [encode_sequence(s, max_len) for s in train_texts]\n",
        "# val_enc   = [encode_sequence(s, max_len) for s in val_texts]\n",
        "# test_enc  = [encode_sequence(s, max_len) for s in test_texts]\n",
        "\n",
        "# train_lab = [encode_labels(l, max_len) for l in train_labels]\n",
        "# val_lab   = [encode_labels(l, max_len) for l in val_labels]\n",
        "# test_lab  = [encode_labels(l, max_len) for l in test_labels]\n",
        "\n",
        "# # =========================================================\n",
        "# # 5. Torch Dataset\n",
        "# # =========================================================\n",
        "# class PSSDataset(Dataset):\n",
        "#     def __init__(self, encodings, labels):\n",
        "#         self.encodings = encodings\n",
        "#         self.labels = labels\n",
        "#     def __len__(self):\n",
        "#         return len(self.encodings)\n",
        "#     def __getitem__(self, idx):\n",
        "#         return {\n",
        "#             \"input_ids\": torch.tensor(self.encodings[idx], dtype=torch.long),\n",
        "#             \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "#         }\n",
        "\n",
        "# train_dataset = PSSDataset(train_enc, train_lab)\n",
        "# val_dataset   = PSSDataset(val_enc, val_lab)\n",
        "# test_dataset  = PSSDataset(test_enc, test_lab)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "# val_loader   = DataLoader(val_dataset, batch_size=32)\n",
        "# test_loader  = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# # =========================================================\n",
        "# # 6. CRNN Model (Convolutional Recurrent Neural Network)\n",
        "# # =========================================================\n",
        "# class CRNNModel(nn.Module):\n",
        "#     def __init__(self, vocab_size, embed_dim, hidden_dim, num_labels, pad_idx=0):\n",
        "#         super(CRNNModel, self).__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "\n",
        "#         # Convolutional layer\n",
        "#         self.conv1 = nn.Conv1d(embed_dim, 64, kernel_size=3, stride=1, padding=1)\n",
        "#         self.conv2 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "#         # GRU layer\n",
        "#         self.gru = nn.GRU(128, hidden_dim, num_layers=1, batch_first=True, bidirectional=True)\n",
        "\n",
        "#         # Fully connected layer\n",
        "#         self.classifier = nn.Linear(hidden_dim * 2, num_labels)\n",
        "\n",
        "#     def forward(self, input_ids, labels=None):\n",
        "#         embeds = self.embedding(input_ids)  # (batch, seq_len, embed_dim)\n",
        "#         embeds = embeds.transpose(1, 2)  # (batch, embed_dim, seq_len) for Conv1d\n",
        "\n",
        "#         # Apply convolution\n",
        "#         conv_out = torch.relu(self.conv1(embeds))\n",
        "#         conv_out = torch.relu(self.conv2(conv_out))\n",
        "\n",
        "#         # Recurrent layer (GRU)\n",
        "#         gru_out, _ = self.gru(conv_out.transpose(1, 2))  # (batch, seq_len, hidden*2)\n",
        "\n",
        "#         logits = self.classifier(gru_out)  # (batch, seq_len, num_labels)\n",
        "\n",
        "#         loss = None\n",
        "#         if labels is not None:\n",
        "#             loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "#             loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "\n",
        "#         return (loss, logits) if loss is not None else logits\n",
        "\n",
        "# # =========================================================\n",
        "# # 7. Training Loop\n",
        "# # =========================================================\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# vocab_size = len(aa_vocab) + 1\n",
        "# embed_dim = 128\n",
        "# hidden_dim = 256\n",
        "# num_labels = len(label_map)\n",
        "\n",
        "# model = CRNNModel(vocab_size, embed_dim, hidden_dim, num_labels, pad_idx=pad_idx).to(device)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# epochs = 10\n",
        "# for epoch in range(epochs):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "#     for batch in train_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         loss, logits = model(input_ids, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "#     avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "#     # Validation\n",
        "#     model.eval()\n",
        "#     val_loss = 0\n",
        "#     with torch.no_grad():\n",
        "#         for batch in val_loader:\n",
        "#             input_ids = batch[\"input_ids\"].to(device)\n",
        "#             labels = batch[\"labels\"].to(device)\n",
        "#             loss, _ = model(input_ids, labels)\n",
        "#             val_loss += loss.item()\n",
        "#     avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_WMwt_R40e8",
        "outputId": "46edccb8-5ce7-4d2a-b32e-878dc3c6b94b"
      },
      "id": "m_WMwt_R40e8",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4072971866.py:11: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  pss_data = pd.read_csv('/content/2022-08-03-ss.processed.csv')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 176490\n",
            "Val size: 22061\n",
            "Test size: 22062\n",
            "Epoch 1/10 | Train Loss: 0.9124 | Val Loss: 0.7644\n",
            "Epoch 2/10 | Train Loss: 0.7190 | Val Loss: 0.7148\n",
            "Epoch 3/10 | Train Loss: 0.6705 | Val Loss: 0.6754\n",
            "Epoch 4/10 | Train Loss: 0.6468 | Val Loss: 0.6687\n",
            "Epoch 5/10 | Train Loss: 0.6318 | Val Loss: 0.6478\n",
            "Epoch 6/10 | Train Loss: 0.6204 | Val Loss: 0.6497\n",
            "Epoch 7/10 | Train Loss: 0.6114 | Val Loss: 0.6389\n",
            "Epoch 8/10 | Train Loss: 0.6028 | Val Loss: 0.6322\n",
            "Epoch 9/10 | Train Loss: 0.5953 | Val Loss: 0.6258\n",
            "Epoch 10/10 | Train Loss: 0.5882 | Val Loss: 0.6193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# model.eval()\n",
        "# all_preds, all_labels = [], []\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch in test_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "\n",
        "#         logits = model(input_ids)\n",
        "#         preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "#         # Flatten but ignore padding (-100 in labels)\n",
        "#         for p, l in zip(preds.cpu().numpy(), labels.cpu().numpy()):\n",
        "#             for pi, li in zip(p, l):\n",
        "#                 if li != -100:\n",
        "#                     all_preds.append(pi)\n",
        "#                     all_labels.append(li)\n",
        "\n",
        "# # Classification report\n",
        "# target_names = list(label_map.keys())\n",
        "# print(classification_report(all_labels, all_preds, target_names=target_names, digits=4))\n",
        "\n",
        "# # Q8 accuracy\n",
        "# q8_accuracy = accuracy_score(all_labels, all_preds)\n",
        "# print(f\"Q8 Accuracy: {q8_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "aFeY1Mjs4ztU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b60002f-d98b-4762-e9cd-fe32dcab1ef4"
      },
      "id": "aFeY1Mjs4ztU",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.7612    0.7751    0.7681    646157\n",
            "           E     0.8399    0.8857    0.8622    563979\n",
            "           H     0.8621    0.9359    0.8975    682331\n",
            "           B     0.7829    0.2871    0.4201     26516\n",
            "           G     0.6621    0.5495    0.6005     72668\n",
            "           I     0.7204    0.1759    0.2827       381\n",
            "           T     0.6565    0.6195    0.6375    255238\n",
            "           S     0.6430    0.4689    0.5423    210849\n",
            "\n",
            "    accuracy                         0.7907   2458119\n",
            "   macro avg     0.7410    0.5872    0.6264   2458119\n",
            "weighted avg     0.7836    0.7907    0.7839   2458119\n",
            "\n",
            "Q8 Accuracy: 0.7907\n"
          ]
        }
      ]
    },
    {
      "id": "52477820-5c1a-47b0-9ac2-c18bb81069ba",
      "cell_type": "markdown",
      "source": [
        "**BiGRU Model**"
      ],
      "metadata": {
        "id": "52477820-5c1a-47b0-9ac2-c18bb81069ba"
      }
    },
    {
      "id": "5434c76d",
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # =========================================================\n",
        "# # 1. Load & Clean Dataset\n",
        "# # =========================================================\n",
        "# pss_data = pd.read_csv('/kaggle/input/maindataaaaaaaaa/2022-08-03-ss.processed.csv')\n",
        "# pss_data_cleaned = pss_data.dropna(subset=['sst8'])\n",
        "\n",
        "# # =========================================================\n",
        "# # 2. Train / Val / Test Split\n",
        "# # =========================================================\n",
        "# label_map = {'C': 0, 'E': 1, 'H': 2, 'B': 3, 'G': 4, 'I': 5, 'T': 6, 'S': 7}\n",
        "\n",
        "\n",
        "# X = pss_data['seq']   # amino acid sequences\n",
        "# y = pss_data['sst8']  # secondary structure labels\n",
        "\n",
        "# train_texts, val_texts, train_labels, val_labels = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# val_texts, test_texts, val_labels, test_labels = train_test_split(val_texts, val_labels, test_size=0.5, random_state=42)\n",
        "\n",
        "# print(\"Train size:\", len(train_texts))\n",
        "# print(\"Val size:\", len(val_texts))\n",
        "# print(\"Test size:\", len(test_texts))\n",
        "\n",
        "# # =========================================================\n",
        "# # 4. Encoding Sequences & Labels\n",
        "# # =========================================================\n",
        "# # Amino acid vocabulary\n",
        "# aa_vocab = {aa: idx+1 for idx, aa in enumerate(\"ACDEFGHIKLMNPQRSTVWY\")}\n",
        "# aa_vocab[\"X\"] = len(aa_vocab) + 1  # unknown amino acid\n",
        "# pad_idx = 0\n",
        "\n",
        "# def encode_sequence(seq, max_len=128):\n",
        "#     ids = [aa_vocab.get(aa, aa_vocab[\"X\"]) for aa in seq]\n",
        "#     ids = ids[:max_len] + [pad_idx] * (max_len - len(ids))\n",
        "#     return ids\n",
        "\n",
        "# def encode_labels(labels, max_len=128):\n",
        "#     ids = [label_map.get(l, -100) for l in labels]\n",
        "#     ids = ids[:max_len] + [-100] * (max_len - len(ids))\n",
        "#     return ids\n",
        "\n",
        "# max_len = 128\n",
        "\n",
        "# train_enc = [encode_sequence(s, max_len) for s in train_texts]\n",
        "# val_enc   = [encode_sequence(s, max_len) for s in val_texts]\n",
        "# test_enc  = [encode_sequence(s, max_len) for s in test_texts]\n",
        "\n",
        "# train_lab = [encode_labels(l, max_len) for l in train_labels]\n",
        "# val_lab   = [encode_labels(l, max_len) for l in val_labels]\n",
        "# test_lab  = [encode_labels(l, max_len) for l in test_labels]\n",
        "\n",
        "# # =========================================================\n",
        "# # 5. Torch Dataset\n",
        "# # =========================================================\n",
        "# class PSSDataset(Dataset):\n",
        "#     def __init__(self, encodings, labels):\n",
        "#         self.encodings = encodings\n",
        "#         self.labels = labels\n",
        "#     def __len__(self):\n",
        "#         return len(self.encodings)\n",
        "#     def __getitem__(self, idx):\n",
        "#         return {\n",
        "#             \"input_ids\": torch.tensor(self.encodings[idx], dtype=torch.long),\n",
        "#             \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "#         }\n",
        "\n",
        "# train_dataset = PSSDataset(train_enc, train_lab)\n",
        "# val_dataset   = PSSDataset(val_enc, val_lab)\n",
        "# test_dataset  = PSSDataset(test_enc, test_lab)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "# val_loader   = DataLoader(val_dataset, batch_size=32)\n",
        "# test_loader  = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# # =========================================================\n",
        "# # 6. BiGRU Model\n",
        "# # =========================================================\n",
        "# class BiGRUModel(nn.Module):\n",
        "#     def __init__(self, vocab_size, embed_dim, hidden_dim, num_labels, pad_idx=0):\n",
        "#         super(BiGRUModel, self).__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "#         self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=1,\n",
        "#                           bidirectional=True, batch_first=True)\n",
        "#         self.classifier = nn.Linear(hidden_dim * 2, num_labels)\n",
        "\n",
        "#     def forward(self, input_ids, labels=None):\n",
        "#         embeds = self.embedding(input_ids)   # (batch, seq_len, embed_dim)\n",
        "#         gru_out, _ = self.gru(embeds)        # (batch, seq_len, hidden*2)\n",
        "#         logits = self.classifier(gru_out)    # (batch, seq_len, num_labels)\n",
        "\n",
        "#         loss = None\n",
        "#         if labels is not None:\n",
        "#             loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "#             loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "\n",
        "#         return (loss, logits) if loss is not None else logits\n",
        "\n",
        "# # =========================================================\n",
        "# # 7. Training Loop\n",
        "# # =========================================================\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# vocab_size = len(aa_vocab) + 1\n",
        "# embed_dim = 128\n",
        "# hidden_dim = 256\n",
        "# num_labels = len(label_map)\n",
        "\n",
        "# model = BiGRUModel(vocab_size, embed_dim, hidden_dim, num_labels, pad_idx=pad_idx).to(device)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# epochs = 5\n",
        "# for epoch in range(epochs):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "#     for batch in train_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         loss, logits = model(input_ids, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "#     avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "#     # Validation\n",
        "#     model.eval()\n",
        "#     val_loss = 0\n",
        "#     with torch.no_grad():\n",
        "#         for batch in val_loader:\n",
        "#             input_ids = batch[\"input_ids\"].to(device)\n",
        "#             labels = batch[\"labels\"].to(device)\n",
        "#             loss, _ = model(input_ids, labels)\n",
        "#             val_loss += loss.item()\n",
        "#     avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-04T17:08:40.171689Z",
          "iopub.status.busy": "2025-09-04T17:08:40.171470Z",
          "iopub.status.idle": "2025-09-04T17:13:43.749232Z",
          "shell.execute_reply": "2025-09-04T17:13:43.748354Z"
        },
        "id": "5434c76d",
        "outputId": "df3eab62-e9fb-4e63-863b-9b6159d93b83",
        "papermill": {
          "duration": 303.586978,
          "end_time": "2025-09-04T17:13:43.753788",
          "exception": false,
          "start_time": "2025-09-04T17:08:40.166810",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_19/267209145.py:11: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  pss_data = pd.read_csv('/kaggle/input/maindataaaaaaaaa/2022-08-03-ss.processed.csv')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 176490\n",
            "Val size: 22061\n",
            "Test size: 22062\n",
            "Epoch 1/5 | Train Loss: 0.9290 | Val Loss: 0.8078\n",
            "Epoch 2/5 | Train Loss: 0.7690 | Val Loss: 0.7564\n",
            "Epoch 3/5 | Train Loss: 0.7313 | Val Loss: 0.7406\n",
            "Epoch 4/5 | Train Loss: 0.7181 | Val Loss: 0.7356\n",
            "Epoch 5/5 | Train Loss: 0.7116 | Val Loss: 0.7303\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "fe2076f2",
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# model.eval()\n",
        "# all_preds, all_labels = [], []\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch in test_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "\n",
        "#         logits = model(input_ids)\n",
        "#         preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "#         # Flatten but ignore padding (-100 in labels)\n",
        "#         for p, l in zip(preds.cpu().numpy(), labels.cpu().numpy()):\n",
        "#             for pi, li in zip(p, l):\n",
        "#                 if li != -100:\n",
        "#                     all_preds.append(pi)\n",
        "#                     all_labels.append(li)\n",
        "\n",
        "# # Classification report\n",
        "# target_names = list(label_map.keys())\n",
        "# print(classification_report(all_labels, all_preds, target_names=target_names, digits=4))\n",
        "\n",
        "# # Q8 accuracy\n",
        "# q8_accuracy = accuracy_score(all_labels, all_preds)\n",
        "# print(f\"Q8 Accuracy: {q8_accuracy:.4f}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-04T17:13:43.762202Z",
          "iopub.status.busy": "2025-09-04T17:13:43.761795Z",
          "iopub.status.idle": "2025-09-04T17:13:52.305215Z",
          "shell.execute_reply": "2025-09-04T17:13:52.304361Z"
        },
        "id": "fe2076f2",
        "papermill": {
          "duration": 8.549011,
          "end_time": "2025-09-04T17:13:52.306519",
          "exception": false,
          "start_time": "2025-09-04T17:13:43.757508",
          "status": "completed"
        },
        "tags": [],
        "outputId": "28fb44ab-6c61-427f-a6b9-275d77b402ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.7121    0.7448    0.7281    646157\n",
            "           E     0.7918    0.8615    0.8252    563979\n",
            "           H     0.8415    0.9027    0.8710    682331\n",
            "           B     0.7604    0.2170    0.3377     26516\n",
            "           G     0.6293    0.4475    0.5230     72668\n",
            "           I     0.6638    0.2021    0.3099       381\n",
            "           T     0.6006    0.5581    0.5786    255238\n",
            "           S     0.5834    0.3875    0.4657    210849\n",
            "\n",
            "    accuracy                         0.7508   2458119\n",
            "   macro avg     0.6978    0.5402    0.5799   2458119\n",
            "weighted avg     0.7417    0.7508    0.7417   2458119\n",
            "\n",
            "Q8 Accuracy: 0.7508\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "38e7e2ba-5e11-4296-a970-212b133961db",
      "cell_type": "markdown",
      "source": [
        "**CNN Model**"
      ],
      "metadata": {
        "id": "38e7e2ba-5e11-4296-a970-212b133961db"
      }
    },
    {
      "id": "731182bb",
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# # =========================================================\n",
        "# # 1. Load & Clean Dataset\n",
        "# # =========================================================\n",
        "# pss_data = pd.read_csv('/kaggle/input/maindataaaaaaaaa/2022-08-03-ss.processed.csv')\n",
        "# pss_data_cleaned = pss_data.dropna(subset=['sst8'])\n",
        "\n",
        "# # =========================================================\n",
        "# # 2. Train / Val / Test Split\n",
        "# # =========================================================\n",
        "# label_map = {'C': 0, 'E': 1, 'H': 2, 'B': 3, 'G': 4, 'I': 5, 'T': 6, 'S': 7}\n",
        "\n",
        "\n",
        "# # =========================================================\n",
        "# # 3. Train / Val / Test Split\n",
        "# # =========================================================\n",
        "# X = pss_data['seq']   # amino acid sequences\n",
        "# y = pss_data['sst8']  # secondary structure labels\n",
        "\n",
        "# train_texts, val_texts, train_labels, val_labels = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# val_texts, test_texts, val_labels, test_labels = train_test_split(val_texts, val_labels, test_size=0.5, random_state=42)\n",
        "\n",
        "# print(\"Train size:\", len(train_texts))\n",
        "# print(\"Val size:\", len(val_texts))\n",
        "# print(\"Test size:\", len(test_texts))\n",
        "\n",
        "# # =========================================================\n",
        "# # 3. Encoding Sequences & Labels\n",
        "# # =========================================================\n",
        "# aa_vocab = {aa: idx+1 for idx, aa in enumerate(\"ACDEFGHIKLMNPQRSTVWY\")}\n",
        "# aa_vocab[\"X\"] = len(aa_vocab) + 1  # unknown amino acid\n",
        "# pad_idx = 0\n",
        "\n",
        "# def encode_sequence(seq, max_len=128):\n",
        "#     ids = [aa_vocab.get(aa, aa_vocab[\"X\"]) for aa in seq]\n",
        "#     ids = ids[:max_len] + [pad_idx] * (max_len - len(ids))\n",
        "#     return ids\n",
        "\n",
        "# def encode_labels(labels, max_len=128):\n",
        "#     ids = [label_map.get(l, -100) for l in labels]\n",
        "#     ids = ids[:max_len] + [-100] * (max_len - len(ids))\n",
        "#     return ids\n",
        "\n",
        "# max_len = 128\n",
        "\n",
        "# train_enc = [encode_sequence(s, max_len) for s in train_texts]\n",
        "# val_enc   = [encode_sequence(s, max_len) for s in val_texts]\n",
        "# test_enc  = [encode_sequence(s, max_len) for s in test_texts]\n",
        "\n",
        "# train_lab = [encode_labels(l, max_len) for l in train_labels]\n",
        "# val_lab   = [encode_labels(l, max_len) for l in val_labels]\n",
        "# test_lab  = [encode_labels(l, max_len) for l in test_labels]\n",
        "\n",
        "# # =========================================================\n",
        "# # 4. Torch Dataset\n",
        "# # =========================================================\n",
        "# class PSSDataset(Dataset):\n",
        "#     def __init__(self, encodings, labels):\n",
        "#         self.encodings = encodings\n",
        "#         self.labels = labels\n",
        "#     def __len__(self):\n",
        "#         return len(self.encodings)\n",
        "#     def __getitem__(self, idx):\n",
        "#         return {\n",
        "#             \"input_ids\": torch.tensor(self.encodings[idx], dtype=torch.long),\n",
        "#             \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "#         }\n",
        "\n",
        "# train_dataset = PSSDataset(train_enc, train_lab)\n",
        "# val_dataset   = PSSDataset(val_enc, val_lab)\n",
        "# test_dataset  = PSSDataset(test_enc, test_lab)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "# val_loader   = DataLoader(val_dataset, batch_size=32)\n",
        "# test_loader  = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# # =========================================================\n",
        "# # 5. CNN Model\n",
        "# # =========================================================\n",
        "# class CNNModel(nn.Module):\n",
        "#     def __init__(self, vocab_size, embed_dim, num_labels, pad_idx=0):\n",
        "#         super(CNNModel, self).__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "#         # 1D Convolution layers\n",
        "#         self.conv1 = nn.Conv1d(embed_dim, 128, kernel_size=3, padding=1)\n",
        "#         self.conv2 = nn.Conv1d(128, 128, kernel_size=3, padding=1)\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.dropout = nn.Dropout(0.3)\n",
        "#         self.classifier = nn.Linear(128, num_labels)\n",
        "\n",
        "#     def forward(self, input_ids, labels=None):\n",
        "#         # input_ids: (batch, seq_len)\n",
        "#         x = self.embedding(input_ids)       # (batch, seq_len, embed_dim)\n",
        "#         x = x.transpose(1, 2)              # (batch, embed_dim, seq_len) for Conv1d\n",
        "#         x = self.relu(self.conv1(x))        # (batch, 128, seq_len)\n",
        "#         x = self.relu(self.conv2(x))        # (batch, 128, seq_len)\n",
        "#         x = x.transpose(1, 2)              # back to (batch, seq_len, 128)\n",
        "#         x = self.dropout(x)\n",
        "#         logits = self.classifier(x)        # (batch, seq_len, num_labels)\n",
        "\n",
        "#         loss = None\n",
        "#         if labels is not None:\n",
        "#             loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "#             loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "\n",
        "#         return (loss, logits) if loss is not None else logits\n",
        "\n",
        "# # =========================================================\n",
        "# # 6. Training Loop\n",
        "# # =========================================================\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# vocab_size = len(aa_vocab) + 1\n",
        "# embed_dim = 128\n",
        "# num_labels = len(label_map)\n",
        "\n",
        "# model = CNNModel(vocab_size, embed_dim, num_labels, pad_idx=pad_idx).to(device)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# epochs = 5\n",
        "# for epoch in range(epochs):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "#     for batch in train_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         loss, logits = model(input_ids, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "#     avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "#     # Validation\n",
        "#     model.eval()\n",
        "#     val_loss = 0\n",
        "#     with torch.no_grad():\n",
        "#         for batch in val_loader:\n",
        "#             input_ids = batch[\"input_ids\"].to(device)\n",
        "#             labels = batch[\"labels\"].to(device)\n",
        "#             loss, _ = model(input_ids, labels)\n",
        "#             val_loss += loss.item()\n",
        "#     avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-04T17:13:52.324086Z",
          "iopub.status.busy": "2025-09-04T17:13:52.323857Z",
          "iopub.status.idle": "2025-09-04T17:15:41.907395Z",
          "shell.execute_reply": "2025-09-04T17:15:41.906635Z"
        },
        "id": "731182bb",
        "papermill": {
          "duration": 109.589524,
          "end_time": "2025-09-04T17:15:41.908663",
          "exception": false,
          "start_time": "2025-09-04T17:13:52.319139",
          "status": "completed"
        },
        "tags": [],
        "outputId": "942f49a5-e85d-452e-f061-4fca5800bffd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_19/1134077729.py:12: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  pss_data = pd.read_csv('/kaggle/input/maindataaaaaaaaa/2022-08-03-ss.processed.csv')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 176490\n",
            "Val size: 22061\n",
            "Test size: 22062\n",
            "Epoch 1/5 | Train Loss: 1.3158 | Val Loss: 1.2569\n",
            "Epoch 2/5 | Train Loss: 1.2729 | Val Loss: 1.2381\n",
            "Epoch 3/5 | Train Loss: 1.2617 | Val Loss: 1.2220\n",
            "Epoch 4/5 | Train Loss: 1.2559 | Val Loss: 1.2185\n",
            "Epoch 5/5 | Train Loss: 1.2519 | Val Loss: 1.2133\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "38cefeff",
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# model.eval()\n",
        "# all_preds, all_labels = [], []\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch in test_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "\n",
        "#         logits = model(input_ids, labels=None)  #  just logits\n",
        "#         preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "#         # Flatten and remove padding\n",
        "#         for p, l in zip(preds.cpu().numpy(), labels.cpu().numpy()):\n",
        "#             for pi, li in zip(p, l):\n",
        "#                 if li != -100:\n",
        "#                     all_preds.append(pi)\n",
        "#                     all_labels.append(li)\n",
        "\n",
        "# # Classification report\n",
        "# target_names = list(label_map.keys())\n",
        "# print(classification_report(all_labels, all_preds, target_names=target_names, digits=4))\n",
        "\n",
        "# # Q8 accuracy\n",
        "# q8_accuracy = accuracy_score(all_labels, all_preds)\n",
        "# print(f\"Q8 Accuracy: {q8_accuracy:.4f}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-04T17:15:41.918366Z",
          "iopub.status.busy": "2025-09-04T17:15:41.918151Z",
          "iopub.status.idle": "2025-09-04T17:15:48.911824Z",
          "shell.execute_reply": "2025-09-04T17:15:48.911025Z"
        },
        "id": "38cefeff",
        "papermill": {
          "duration": 6.999844,
          "end_time": "2025-09-04T17:15:48.912963",
          "exception": false,
          "start_time": "2025-09-04T17:15:41.913119",
          "status": "completed"
        },
        "tags": [],
        "outputId": "fc507bff-8c5c-4460-a519-63b43ba61965"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.5248    0.6276    0.5716    646157\n",
            "           E     0.5922    0.6111    0.6015    563979\n",
            "           H     0.5529    0.7333    0.6305    682331\n",
            "           B     0.8668    0.0672    0.1248     26516\n",
            "           G     0.7481    0.1018    0.1792     72668\n",
            "           I     0.0000    0.0000    0.0000       381\n",
            "           T     0.5114    0.3026    0.3802    255238\n",
            "           S     0.6371    0.1071    0.1834    210849\n",
            "\n",
            "    accuracy                         0.5531   2458119\n",
            "   macro avg     0.5542    0.3188    0.3339   2458119\n",
            "weighted avg     0.5665    0.5531    0.5251   2458119\n",
            "\n",
            "Q8 Accuracy: 0.5531\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "da43f3a7",
      "cell_type": "markdown",
      "source": [
        "**BiLSTM Model**"
      ],
      "metadata": {
        "id": "da43f3a7",
        "papermill": {
          "duration": 0.004126,
          "end_time": "2025-09-04T17:15:48.921918",
          "exception": false,
          "start_time": "2025-09-04T17:15:48.917792",
          "status": "completed"
        },
        "tags": []
      }
    },
    {
      "id": "1c128003",
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# # =========================================================\n",
        "# # 1. Load & Clean Dataset\n",
        "# # =========================================================\n",
        "# pss_data = pd.read_csv('/kaggle/input/maindataaaaaaaaa/2022-08-03-ss.processed.csv')\n",
        "# pss_data_cleaned = pss_data.dropna(subset=['sst8'])\n",
        "\n",
        "# # =========================================================\n",
        "# # 2. Train / Val / Test Split\n",
        "# # =========================================================\n",
        "# label_map = {'C': 0, 'E': 1, 'H': 2, 'B': 3, 'G': 4, 'I': 5, 'T': 6, 'S': 7}\n",
        "\n",
        "# X = pss_data_cleaned['seq']\n",
        "# y = pss_data_cleaned['sst8']\n",
        "\n",
        "# train_texts, val_texts, train_labels, val_labels = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# val_texts, test_texts, val_labels, test_labels = train_test_split(val_texts, val_labels, test_size=0.5, random_state=42)\n",
        "\n",
        "# print(\"Train size:\", len(train_texts))\n",
        "# print(\"Val size:\", len(val_texts))\n",
        "# print(\"Test size:\", len(test_texts))\n",
        "\n",
        "# # =========================================================\n",
        "# # 3. Encoding Sequences & Labels\n",
        "# # =========================================================\n",
        "# aa_vocab = {aa: idx+1 for idx, aa in enumerate(\"ACDEFGHIKLMNPQRSTVWY\")}\n",
        "# aa_vocab[\"X\"] = len(aa_vocab) + 1  # unknown amino acid\n",
        "# pad_idx = 0\n",
        "\n",
        "# def encode_sequence(seq, max_len=128):\n",
        "#     ids = [aa_vocab.get(aa, aa_vocab[\"X\"]) for aa in seq]\n",
        "#     ids = ids[:max_len] + [pad_idx] * (max_len - len(ids))\n",
        "#     return ids\n",
        "\n",
        "# def encode_labels(labels, max_len=128):\n",
        "#     ids = [label_map.get(l, -100) for l in labels]\n",
        "#     ids = ids[:max_len] + [-100] * (max_len - len(ids))\n",
        "#     return ids\n",
        "\n",
        "# max_len = 128\n",
        "\n",
        "# train_enc = [encode_sequence(s, max_len) for s in train_texts]\n",
        "# val_enc   = [encode_sequence(s, max_len) for s in val_texts]\n",
        "# test_enc  = [encode_sequence(s, max_len) for s in test_texts]\n",
        "\n",
        "# train_lab = [encode_labels(l, max_len) for l in train_labels]\n",
        "# val_lab   = [encode_labels(l, max_len) for l in val_labels]\n",
        "# test_lab  = [encode_labels(l, max_len) for l in test_labels]\n",
        "\n",
        "# # =========================================================\n",
        "# # 4. Torch Dataset\n",
        "# # =========================================================\n",
        "# class PSSDataset(Dataset):\n",
        "#     def __init__(self, encodings, labels):\n",
        "#         self.encodings = encodings\n",
        "#         self.labels = labels\n",
        "#     def __len__(self):\n",
        "#         return len(self.encodings)\n",
        "#     def __getitem__(self, idx):\n",
        "#         return {\n",
        "#             \"input_ids\": torch.tensor(self.encodings[idx], dtype=torch.long),\n",
        "#             \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "#         }\n",
        "\n",
        "# train_dataset = PSSDataset(train_enc, train_lab)\n",
        "# val_dataset   = PSSDataset(val_enc, val_lab)\n",
        "# test_dataset  = PSSDataset(test_enc, test_lab)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "# val_loader   = DataLoader(val_dataset, batch_size=32)\n",
        "# test_loader  = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# # =========================================================\n",
        "# # 5. BiLSTM Model\n",
        "# # =========================================================\n",
        "# class BiLSTMModel(nn.Module):\n",
        "#     def __init__(self, vocab_size, embed_dim, hidden_dim, num_labels, pad_idx=0):\n",
        "#         super(BiLSTMModel, self).__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "#         self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=1,\n",
        "#                             bidirectional=True, batch_first=True)\n",
        "#         self.classifier = nn.Linear(hidden_dim * 2, num_labels)\n",
        "\n",
        "#     def forward(self, input_ids, labels=None):\n",
        "#         embeds = self.embedding(input_ids)        # (batch, seq_len, embed_dim)\n",
        "#         lstm_out, _ = self.lstm(embeds)          # (batch, seq_len, hidden*2)\n",
        "#         logits = self.classifier(lstm_out)       # (batch, seq_len, num_labels)\n",
        "\n",
        "#         loss = None\n",
        "#         if labels is not None:\n",
        "#             loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "#             loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "\n",
        "#         return (loss, logits) if loss is not None else logits\n",
        "\n",
        "# # =========================================================\n",
        "# # 6. Training Loop\n",
        "# # =========================================================\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# vocab_size = len(aa_vocab) + 1\n",
        "# embed_dim = 128\n",
        "# hidden_dim = 256\n",
        "# num_labels = len(label_map)\n",
        "\n",
        "# model = BiLSTMModel(vocab_size, embed_dim, hidden_dim, num_labels, pad_idx=pad_idx).to(device)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# epochs = 5\n",
        "# for epoch in range(epochs):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "#     for batch in train_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         loss, logits = model(input_ids, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "#     avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "#     # Validation\n",
        "#     model.eval()\n",
        "#     val_loss = 0\n",
        "#     with torch.no_grad():\n",
        "#         for batch in val_loader:\n",
        "#             input_ids = batch[\"input_ids\"].to(device)\n",
        "#             labels = batch[\"labels\"].to(device)\n",
        "#             loss, _ = model(input_ids, labels)\n",
        "#             val_loss += loss.item()\n",
        "#     avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "# # =========================================================\n",
        "# # 7. Evaluation: Q8 Accuracy & Classification Report\n",
        "# # =========================================================\n",
        "# model.eval()\n",
        "# all_preds, all_labels = [], []\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch in test_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "\n",
        "#         logits = model(input_ids)  #  BiLSTM returns only logits here\n",
        "#         preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "#         for p, l in zip(preds.cpu().numpy(), labels.cpu().numpy()):\n",
        "#             for pi, li in zip(p, l):\n",
        "#                 if li != -100:\n",
        "#                     all_preds.append(pi)\n",
        "#                     all_labels.append(li)\n",
        "\n",
        "# target_names = list(label_map.keys())\n",
        "# print(classification_report(all_labels, all_preds, target_names=target_names, digits=4))\n",
        "\n",
        "# q8_accuracy = accuracy_score(all_labels, all_preds)\n",
        "# print(f\"Q8 Accuracy: {q8_accuracy:.4f}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-04T17:15:48.932351Z",
          "iopub.status.busy": "2025-09-04T17:15:48.932128Z",
          "iopub.status.idle": "2025-09-04T17:21:42.830450Z",
          "shell.execute_reply": "2025-09-04T17:21:42.829536Z"
        },
        "id": "1c128003",
        "papermill": {
          "duration": 353.90549,
          "end_time": "2025-09-04T17:21:42.831811",
          "exception": false,
          "start_time": "2025-09-04T17:15:48.926321",
          "status": "completed"
        },
        "tags": [],
        "outputId": "2ae55f29-b738-4029-a96e-79d066cb0783"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_19/244902210.py:12: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  pss_data = pd.read_csv('/kaggle/input/maindataaaaaaaaa/2022-08-03-ss.processed.csv')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 176490\n",
            "Val size: 22061\n",
            "Test size: 22062\n",
            "Epoch 1/5 | Train Loss: 0.9270 | Val Loss: 0.7939\n",
            "Epoch 2/5 | Train Loss: 0.7450 | Val Loss: 0.7245\n",
            "Epoch 3/5 | Train Loss: 0.6902 | Val Loss: 0.6953\n",
            "Epoch 4/5 | Train Loss: 0.6622 | Val Loss: 0.6790\n",
            "Epoch 5/5 | Train Loss: 0.6463 | Val Loss: 0.6665\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.7182    0.7886    0.7517    646157\n",
            "           E     0.8230    0.8700    0.8458    563979\n",
            "           H     0.8648    0.9089    0.8863    682331\n",
            "           B     0.7782    0.2634    0.3936     26516\n",
            "           G     0.6694    0.4889    0.5651     72668\n",
            "           I     0.6053    0.2415    0.3452       381\n",
            "           T     0.6482    0.5901    0.6178    255238\n",
            "           S     0.6418    0.4285    0.5139    210849\n",
            "\n",
            "    accuracy                         0.7746   2458119\n",
            "   macro avg     0.7186    0.5725    0.6149   2458119\n",
            "weighted avg     0.7683    0.7746    0.7669   2458119\n",
            "\n",
            "Q8 Accuracy: 0.7746\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "e7387f13",
      "cell_type": "markdown",
      "source": [
        "**DenseNet Model**"
      ],
      "metadata": {
        "id": "e7387f13",
        "papermill": {
          "duration": 0.004475,
          "end_time": "2025-09-04T17:21:42.841264",
          "exception": false,
          "start_time": "2025-09-04T17:21:42.836789",
          "status": "completed"
        },
        "tags": []
      }
    },
    {
      "id": "ed6e6820",
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# # =========================================================\n",
        "# # 1. Load & Clean Dataset\n",
        "# # =========================================================\n",
        "# pss_data = pd.read_csv('/kaggle/input/maindataaaaaaaaa/2022-08-03-ss.processed.csv')\n",
        "# pss_data_cleaned = pss_data.dropna(subset=['sst8'])\n",
        "\n",
        "# # =========================================================\n",
        "# # 2. Train / Val / Test Split\n",
        "# # =========================================================\n",
        "# label_map = {'C': 0, 'E': 1, 'H': 2, 'B': 3, 'G': 4, 'I': 5, 'T': 6, 'S': 7}\n",
        "\n",
        "# X = pss_data_cleaned['seq']\n",
        "# y = pss_data_cleaned['sst8']\n",
        "\n",
        "# train_texts, val_texts, train_labels, val_labels = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# val_texts, test_texts, val_labels, test_labels = train_test_split(val_texts, val_labels, test_size=0.5, random_state=42)\n",
        "\n",
        "# print(\"Train size:\", len(train_texts))\n",
        "# print(\"Val size:\", len(val_texts))\n",
        "# print(\"Test size:\", len(test_texts))\n",
        "\n",
        "# # =========================================================\n",
        "# # 3. Encoding Sequences & Labels\n",
        "# # =========================================================\n",
        "# aa_vocab = {aa: idx+1 for idx, aa in enumerate(\"ACDEFGHIKLMNPQRSTVWY\")}\n",
        "# aa_vocab[\"X\"] = len(aa_vocab) + 1\n",
        "# pad_idx = 0\n",
        "\n",
        "# def encode_sequence(seq, max_len=128):\n",
        "#     ids = [aa_vocab.get(aa, aa_vocab[\"X\"]) for aa in seq]\n",
        "#     ids = ids[:max_len] + [pad_idx] * (max_len - len(ids))\n",
        "#     return ids\n",
        "\n",
        "# def encode_labels(labels, max_len=128):\n",
        "#     ids = [label_map.get(l, -100) for l in labels]\n",
        "#     ids = ids[:max_len] + [-100] * (max_len - len(ids))\n",
        "#     return ids\n",
        "\n",
        "# max_len = 128\n",
        "# train_enc = [encode_sequence(s, max_len) for s in train_texts]\n",
        "# val_enc   = [encode_sequence(s, max_len) for s in val_texts]\n",
        "# test_enc  = [encode_sequence(s, max_len) for s in test_texts]\n",
        "\n",
        "# train_lab = [encode_labels(l, max_len) for l in train_labels]\n",
        "# val_lab   = [encode_labels(l, max_len) for l in val_labels]\n",
        "# test_lab  = [encode_labels(l, max_len) for l in test_labels]\n",
        "\n",
        "# # =========================================================\n",
        "# # 4. Torch Dataset\n",
        "# # =========================================================\n",
        "# class PSSDataset(Dataset):\n",
        "#     def __init__(self, encodings, labels):\n",
        "#         self.encodings = encodings\n",
        "#         self.labels = labels\n",
        "#     def __len__(self):\n",
        "#         return len(self.encodings)\n",
        "#     def __getitem__(self, idx):\n",
        "#         return {\n",
        "#             \"input_ids\": torch.tensor(self.encodings[idx], dtype=torch.long),\n",
        "#             \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "#         }\n",
        "\n",
        "# train_dataset = PSSDataset(train_enc, train_lab)\n",
        "# val_dataset   = PSSDataset(val_enc, val_lab)\n",
        "# test_dataset  = PSSDataset(test_enc, test_lab)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "# val_loader   = DataLoader(val_dataset, batch_size=32)\n",
        "# test_loader  = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# # =========================================================\n",
        "# # 5. DenseNet1D Model\n",
        "# # =========================================================\n",
        "# class DenseBlock(nn.Module):\n",
        "#     def __init__(self, in_channels, growth_rate):\n",
        "#         super(DenseBlock, self).__init__()\n",
        "#         self.conv1 = nn.Conv1d(in_channels, growth_rate, kernel_size=3, padding=1)\n",
        "#         self.relu = nn.ReLU()\n",
        "#     def forward(self, x):\n",
        "#         out = self.relu(self.conv1(x))\n",
        "#         out = torch.cat([x, out], dim=1)  # Dense connection\n",
        "#         return out\n",
        "\n",
        "# class DenseNet1D(nn.Module):\n",
        "#     def __init__(self, vocab_size, embed_dim, num_labels, growth_rate=32, num_blocks=3, pad_idx=0):\n",
        "#         super(DenseNet1D, self).__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "#         self.blocks = nn.ModuleList()\n",
        "#         channels = embed_dim\n",
        "#         for _ in range(num_blocks):\n",
        "#             self.blocks.append(DenseBlock(channels, growth_rate))\n",
        "#             channels += growth_rate\n",
        "#         self.dropout = nn.Dropout(0.3)\n",
        "#         self.classifier = nn.Linear(channels, num_labels)\n",
        "#     def forward(self, input_ids, labels=None):\n",
        "#         x = self.embedding(input_ids)        # (batch, seq_len, embed_dim)\n",
        "#         x = x.transpose(1, 2)                # (batch, embed_dim, seq_len)\n",
        "#         for block in self.blocks:\n",
        "#             x = block(x)\n",
        "#         x = x.transpose(1, 2)                # back to (batch, seq_len, channels)\n",
        "#         x = self.dropout(x)\n",
        "#         logits = self.classifier(x)          # (batch, seq_len, num_labels)\n",
        "\n",
        "#         loss = None\n",
        "#         if labels is not None:\n",
        "#             loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "#             loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "#         return (loss, logits) if loss is not None else logits\n",
        "\n",
        "# # =========================================================\n",
        "# # 6. Training Loop\n",
        "# # =========================================================\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# vocab_size = len(aa_vocab) + 1\n",
        "# embed_dim = 128\n",
        "# num_labels = len(label_map)\n",
        "\n",
        "# model = DenseNet1D(vocab_size, embed_dim, num_labels).to(device)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "# epochs = 5\n",
        "\n",
        "# for epoch in range(epochs):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "#     for batch in train_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss, logits = model(input_ids, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "#     avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "#     # Validation\n",
        "#     model.eval()\n",
        "#     val_loss = 0\n",
        "#     with torch.no_grad():\n",
        "#         for batch in val_loader:\n",
        "#             input_ids = batch[\"input_ids\"].to(device)\n",
        "#             labels = batch[\"labels\"].to(device)\n",
        "#             loss, _ = model(input_ids, labels)\n",
        "#             val_loss += loss.item()\n",
        "#     avg_val_loss = val_loss / len(val_loader)\n",
        "#     print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "# # =========================================================\n",
        "# # 7. Evaluation: Q8 Accuracy & Classification Report\n",
        "# # =========================================================\n",
        "# model.eval()\n",
        "# all_preds, all_labels = [], []\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch in test_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "#         logits = model(input_ids)\n",
        "#         preds = torch.argmax(logits, dim=-1)\n",
        "#         for p, l in zip(preds.cpu().numpy(), labels.cpu().numpy()):\n",
        "#             for pi, li in zip(p, l):\n",
        "#                 if li != -100:\n",
        "#                     all_preds.append(pi)\n",
        "#                     all_labels.append(li)\n",
        "\n",
        "# target_names = list(label_map.keys())\n",
        "# print(classification_report(all_labels, all_preds, target_names=target_names, digits=4))\n",
        "# q8_accuracy = accuracy_score(all_labels, all_preds)\n",
        "# print(f\"Q8 Accuracy: {q8_accuracy:.4f}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-04T17:21:42.851497Z",
          "iopub.status.busy": "2025-09-04T17:21:42.851261Z",
          "iopub.status.idle": "2025-09-04T17:24:07.272467Z",
          "shell.execute_reply": "2025-09-04T17:24:07.271587Z"
        },
        "id": "ed6e6820",
        "papermill": {
          "duration": 144.427956,
          "end_time": "2025-09-04T17:24:07.273743",
          "exception": false,
          "start_time": "2025-09-04T17:21:42.845787",
          "status": "completed"
        },
        "tags": [],
        "outputId": "c0f0f8f0-de6f-49fd-d658-6c3a0b3ae3c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_19/3184646567.py:12: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  pss_data = pd.read_csv('/kaggle/input/maindataaaaaaaaa/2022-08-03-ss.processed.csv')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 176490\n",
            "Val size: 22061\n",
            "Test size: 22062\n",
            "Epoch 1/5 | Train Loss: 1.3102 | Val Loss: 1.2511\n",
            "Epoch 2/5 | Train Loss: 1.2718 | Val Loss: 1.2441\n",
            "Epoch 3/5 | Train Loss: 1.2624 | Val Loss: 1.2305\n",
            "Epoch 4/5 | Train Loss: 1.2579 | Val Loss: 1.2354\n",
            "Epoch 5/5 | Train Loss: 1.2551 | Val Loss: 1.2291\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.5530    0.5615    0.5572    646157\n",
            "           E     0.5850    0.6032    0.5940    563979\n",
            "           H     0.5430    0.7950    0.6453    682331\n",
            "           B     0.9524    0.0030    0.0060     26516\n",
            "           G     0.7612    0.0596    0.1105     72668\n",
            "           I     0.0000    0.0000    0.0000       381\n",
            "           T     0.4240    0.3296    0.3709    255238\n",
            "           S     0.4908    0.0404    0.0747    210849\n",
            "\n",
            "    accuracy                         0.5462   2458119\n",
            "   macro avg     0.5387    0.2990    0.2948   2458119\n",
            "weighted avg     0.5492    0.5462    0.5101   2458119\n",
            "\n",
            "Q8 Accuracy: 0.5462\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "6be0ca30",
      "cell_type": "markdown",
      "source": [
        "**VGG Model**"
      ],
      "metadata": {
        "id": "6be0ca30",
        "papermill": {
          "duration": 0.004901,
          "end_time": "2025-09-04T17:24:07.284242",
          "exception": false,
          "start_time": "2025-09-04T17:24:07.279341",
          "status": "completed"
        },
        "tags": []
      }
    },
    {
      "id": "d88625c6",
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# # =========================================================\n",
        "# # 1. Load & Clean Dataset\n",
        "# # =========================================================\n",
        "# pss_data = pd.read_csv('/kaggle/input/maindataaaaaaaaa/2022-08-03-ss.processed.csv')\n",
        "# pss_data_cleaned = pss_data.dropna(subset=['sst8'])\n",
        "\n",
        "# # =========================================================\n",
        "# # 2. Train / Val / Test Split\n",
        "# # =========================================================\n",
        "# label_map = {'C': 0, 'E': 1, 'H': 2, 'B': 3, 'G': 4, 'I': 5, 'T': 6, 'S': 7}\n",
        "\n",
        "# X = pss_data_cleaned['seq']\n",
        "# y = pss_data_cleaned['sst8']\n",
        "\n",
        "# train_texts, val_texts, train_labels, val_labels = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# val_texts, test_texts, val_labels, test_labels = train_test_split(val_texts, val_labels, test_size=0.5, random_state=42)\n",
        "\n",
        "# print(\"Train size:\", len(train_texts))\n",
        "# print(\"Val size:\", len(val_texts))\n",
        "# print(\"Test size:\", len(test_texts))\n",
        "\n",
        "# # =========================================================\n",
        "# # 3. Encoding Sequences & Labels\n",
        "# # =========================================================\n",
        "# aa_vocab = {aa: idx+1 for idx, aa in enumerate(\"ACDEFGHIKLMNPQRSTVWY\")}\n",
        "# aa_vocab[\"X\"] = len(aa_vocab) + 1\n",
        "# pad_idx = 0\n",
        "\n",
        "# def encode_sequence(seq, max_len=128):\n",
        "#     ids = [aa_vocab.get(aa, aa_vocab[\"X\"]) for aa in seq]\n",
        "#     ids = ids[:max_len] + [pad_idx] * (max_len - len(ids))\n",
        "#     return ids\n",
        "\n",
        "# def encode_labels(labels, max_len=128):\n",
        "#     ids = [label_map.get(l, -100) for l in labels]\n",
        "#     ids = ids[:max_len] + [-100] * (max_len - len(ids))\n",
        "#     return ids\n",
        "\n",
        "# max_len = 128\n",
        "# train_enc = [encode_sequence(s, max_len) for s in train_texts]\n",
        "# val_enc   = [encode_sequence(s, max_len) for s in val_texts]\n",
        "# test_enc  = [encode_sequence(s, max_len) for s in test_texts]\n",
        "\n",
        "# train_lab = [encode_labels(l, max_len) for l in train_labels]\n",
        "# val_lab   = [encode_labels(l, max_len) for l in val_labels]\n",
        "# test_lab  = [encode_labels(l, max_len) for l in test_labels]\n",
        "\n",
        "# # =========================================================\n",
        "# # 4. Torch Dataset\n",
        "# # =========================================================\n",
        "# class PSSDataset(Dataset):\n",
        "#     def __init__(self, encodings, labels):\n",
        "#         self.encodings = encodings\n",
        "#         self.labels = labels\n",
        "#     def __len__(self):\n",
        "#         return len(self.encodings)\n",
        "#     def __getitem__(self, idx):\n",
        "#         return {\n",
        "#             \"input_ids\": torch.tensor(self.encodings[idx], dtype=torch.long),\n",
        "#             \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "#         }\n",
        "\n",
        "# train_dataset = PSSDataset(train_enc, train_lab)\n",
        "# val_dataset   = PSSDataset(val_enc, val_lab)\n",
        "# test_dataset  = PSSDataset(test_enc, test_lab)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "# val_loader   = DataLoader(val_dataset, batch_size=32)\n",
        "# test_loader  = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# # =========================================================\n",
        "# # 5. VGG16-1D Model (with padding to maintain seq_len)\n",
        "# # =========================================================\n",
        "# class VGG16_1D(nn.Module):\n",
        "#     def __init__(self, vocab_size, embed_dim, num_labels, pad_idx=0):\n",
        "#         super(VGG16_1D, self).__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "\n",
        "#         self.features = nn.Sequential(\n",
        "#             # Block 1\n",
        "#             nn.Conv1d(embed_dim, 64, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
        "#             nn.Conv1d(64, 64, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
        "#             nn.MaxPool1d(kernel_size=2, stride=1, padding=1),\n",
        "#             # Block 2\n",
        "#             nn.Conv1d(64, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
        "#             nn.Conv1d(128, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
        "#             nn.MaxPool1d(kernel_size=2, stride=1, padding=1),\n",
        "#             # Block 3\n",
        "#             nn.Conv1d(128, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
        "#             nn.Conv1d(256, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
        "#             nn.Conv1d(256, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
        "#             nn.MaxPool1d(kernel_size=2, stride=1, padding=1),\n",
        "#             # Block 4\n",
        "#             nn.Conv1d(256, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
        "#             nn.Conv1d(512, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
        "#             nn.Conv1d(512, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
        "#             nn.MaxPool1d(kernel_size=2, stride=1, padding=1),\n",
        "#             # Block 5\n",
        "#             nn.Conv1d(512, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
        "#             nn.Conv1d(512, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
        "#             nn.Conv1d(512, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
        "#             nn.MaxPool1d(kernel_size=2, stride=1, padding=1)\n",
        "#         )\n",
        "\n",
        "#         self.dropout = nn.Dropout(0.3)\n",
        "#         self.classifier = nn.Linear(512, num_labels)\n",
        "\n",
        "#     def forward(self, input_ids, labels=None):\n",
        "#       x = self.embedding(input_ids)    # (batch, seq_len, embed_dim)\n",
        "#       x = x.transpose(1, 2)            # (batch, embed_dim, seq_len)\n",
        "#       x = self.features(x)\n",
        "#       x = x.transpose(1, 2)            # (batch, seq_len, channels)\n",
        "#       x = self.dropout(x)\n",
        "#       logits = self.classifier(x)\n",
        "\n",
        "#       # Trim logits to max_len if necessary\n",
        "#       logits = logits[:, :max_len, :]\n",
        "\n",
        "#       loss = None\n",
        "#       if labels is not None:\n",
        "#           loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "#           loss = loss_fct(logits.reshape(-1, logits.size(-1)), labels.reshape(-1))\n",
        "#       return (loss, logits) if loss is not None else logits\n",
        "\n",
        "\n",
        "# # =========================================================\n",
        "# # 6. Training Loop\n",
        "# # =========================================================\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# vocab_size = len(aa_vocab) + 1\n",
        "# embed_dim = 128\n",
        "# num_labels = len(label_map)\n",
        "\n",
        "# model = VGG16_1D(vocab_size, embed_dim, num_labels).to(device)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "# epochs = 5\n",
        "\n",
        "# for epoch in range(epochs):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "#     for batch in train_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss, logits = model(input_ids, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "#     avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "#     # Validation\n",
        "#     model.eval()\n",
        "#     val_loss = 0\n",
        "#     with torch.no_grad():\n",
        "#         for batch in val_loader:\n",
        "#             input_ids = batch[\"input_ids\"].to(device)\n",
        "#             labels = batch[\"labels\"].to(device)\n",
        "#             loss, _ = model(input_ids, labels)\n",
        "#             val_loss += loss.item()\n",
        "#     avg_val_loss = val_loss / len(val_loader)\n",
        "#     print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-04T17:24:07.295624Z",
          "iopub.status.busy": "2025-09-04T17:24:07.295406Z",
          "iopub.status.idle": "2025-09-04T17:45:34.855645Z",
          "shell.execute_reply": "2025-09-04T17:45:34.854789Z"
        },
        "id": "d88625c6",
        "papermill": {
          "duration": 1287.57316,
          "end_time": "2025-09-04T17:45:34.862610",
          "exception": false,
          "start_time": "2025-09-04T17:24:07.289450",
          "status": "completed"
        },
        "tags": [],
        "outputId": "d30d3e6c-c5b7-4ec1-ee76-5aa797cd6881"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_19/2323266229.py:12: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  pss_data = pd.read_csv('/kaggle/input/maindataaaaaaaaa/2022-08-03-ss.processed.csv')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 176490\n",
            "Val size: 22061\n",
            "Test size: 22062\n",
            "Epoch 1/5 | Train Loss: 1.0649 | Val Loss: 0.9101\n",
            "Epoch 2/5 | Train Loss: 0.8722 | Val Loss: 0.8405\n",
            "Epoch 3/5 | Train Loss: 0.8162 | Val Loss: 0.8038\n",
            "Epoch 4/5 | Train Loss: 0.7874 | Val Loss: 0.7836\n",
            "Epoch 5/5 | Train Loss: 0.7687 | Val Loss: 0.7678\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "e55e4566",
      "cell_type": "code",
      "source": [
        "# model.eval()\n",
        "# all_preds, all_labels = [], []\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch in test_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "\n",
        "#         logits = model(input_ids, labels=None)  # <-- fixed\n",
        "#         preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "#         for p, l in zip(preds.cpu().numpy(), labels.cpu().numpy()):\n",
        "#             for pi, li in zip(p, l):\n",
        "#                 if li != -100:  # ignore padding\n",
        "#                     all_preds.append(pi)\n",
        "#                     all_labels.append(li)\n",
        "\n",
        "# from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# target_names = list(label_map.keys())\n",
        "# print(classification_report(all_labels, all_preds, target_names=target_names, digits=4))\n",
        "# q8_accuracy = accuracy_score(all_labels, all_preds)\n",
        "# print(f\"Q8 Accuracy: {q8_accuracy:.4f}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-04T17:45:34.875020Z",
          "iopub.status.busy": "2025-09-04T17:45:34.874786Z",
          "iopub.status.idle": "2025-09-04T17:45:48.506412Z",
          "shell.execute_reply": "2025-09-04T17:45:48.505566Z"
        },
        "id": "e55e4566",
        "papermill": {
          "duration": 13.639533,
          "end_time": "2025-09-04T17:45:48.507992",
          "exception": false,
          "start_time": "2025-09-04T17:45:34.868459",
          "status": "completed"
        },
        "tags": [],
        "outputId": "cb1728b2-7374-4ade-d72d-63b1f7d5773e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.6555    0.7564    0.7024    646157\n",
            "           E     0.7962    0.8086    0.8024    563979\n",
            "           H     0.8178    0.8997    0.8568    682331\n",
            "           B     0.8004    0.2380    0.3670     26516\n",
            "           G     0.6949    0.3766    0.4885     72668\n",
            "           I     0.5620    0.2021    0.2973       381\n",
            "           T     0.5864    0.5571    0.5714    255238\n",
            "           S     0.6658    0.3131    0.4259    210849\n",
            "\n",
            "    accuracy                         0.7325   2458119\n",
            "   macro avg     0.6974    0.5190    0.5639   2458119\n",
            "weighted avg     0.7293    0.7325    0.7209   2458119\n",
            "\n",
            "Q8 Accuracy: 0.7325\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "5fba4d05",
      "cell_type": "markdown",
      "source": [
        "**CNN+BiLSTM Model**"
      ],
      "metadata": {
        "id": "5fba4d05",
        "papermill": {
          "duration": 0.005479,
          "end_time": "2025-09-04T17:45:48.519703",
          "exception": false,
          "start_time": "2025-09-04T17:45:48.514224",
          "status": "completed"
        },
        "tags": []
      }
    },
    {
      "id": "d4c4b811",
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# # =========================================================\n",
        "# # 1. Load & Clean Dataset\n",
        "# # =========================================================\n",
        "# pss_data = pd.read_csv('/kaggle/input/maindataaaaaaaaa/2022-08-03-ss.processed.csv')\n",
        "# pss_data_cleaned = pss_data.dropna(subset=['sst8'])\n",
        "\n",
        "# # =========================================================\n",
        "# # 2. Train / Val / Test Split\n",
        "# # =========================================================\n",
        "# label_map = {'C': 0, 'E': 1, 'H': 2, 'B': 3, 'G': 4, 'I': 5, 'T': 6, 'S': 7}\n",
        "\n",
        "# # =========================================================\n",
        "# # 3. Train / Val / Test Split\n",
        "# # =========================================================\n",
        "# X = pss_data['seq']   # amino acid sequences\n",
        "# y = pss_data['sst8']  # secondary structure labels\n",
        "\n",
        "# train_texts, val_texts, train_labels, val_labels = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# val_texts, test_texts, val_labels, test_labels = train_test_split(val_texts, val_labels, test_size=0.5, random_state=42)\n",
        "\n",
        "# print(\"Train size:\", len(train_texts))\n",
        "# print(\"Val size:\", len(val_texts))\n",
        "# print(\"Test size:\", len(test_texts))\n",
        "\n",
        "# # =========================================================\n",
        "# # 3. Encoding Sequences & Labels\n",
        "# # =========================================================\n",
        "# aa_vocab = {aa: idx+1 for idx, aa in enumerate(\"ACDEFGHIKLMNPQRSTVWY\")}\n",
        "# aa_vocab[\"X\"] = len(aa_vocab) + 1\n",
        "# pad_idx = 0\n",
        "# max_len = 128\n",
        "\n",
        "# def encode_sequence(seq, max_len=max_len):\n",
        "#     ids = [aa_vocab.get(aa, aa_vocab[\"X\"]) for aa in seq]\n",
        "#     ids = ids[:max_len] + [pad_idx] * (max_len - len(ids))\n",
        "#     return ids\n",
        "\n",
        "# def encode_labels(labels, max_len=max_len):\n",
        "#     ids = [label_map.get(l, -100) for l in labels]\n",
        "#     ids = ids[:max_len] + [-100] * (max_len - len(ids))\n",
        "#     return ids\n",
        "\n",
        "# train_enc = [encode_sequence(s) for s in train_texts]\n",
        "# val_enc   = [encode_sequence(s) for s in val_texts]\n",
        "# test_enc  = [encode_sequence(s) for s in test_texts]\n",
        "\n",
        "# train_lab = [encode_labels(l) for l in train_labels]\n",
        "# val_lab   = [encode_labels(l) for l in val_labels]\n",
        "# test_lab  = [encode_labels(l) for l in test_labels]\n",
        "\n",
        "# # =========================================================\n",
        "# # 4. Torch Dataset\n",
        "# # =========================================================\n",
        "# class PSSDataset(Dataset):\n",
        "#     def __init__(self, encodings, labels):\n",
        "#         self.encodings = encodings\n",
        "#         self.labels = labels\n",
        "#     def __len__(self):\n",
        "#         return len(self.encodings)\n",
        "#     def __getitem__(self, idx):\n",
        "#         return {\n",
        "#             \"input_ids\": torch.tensor(self.encodings[idx], dtype=torch.long),\n",
        "#             \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "#         }\n",
        "\n",
        "# train_dataset = PSSDataset(train_enc, train_lab)\n",
        "# val_dataset   = PSSDataset(val_enc, val_lab)\n",
        "# test_dataset  = PSSDataset(test_enc, test_lab)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True)\n",
        "# val_loader   = DataLoader(val_dataset, batch_size=32, pin_memory=True)\n",
        "# test_loader  = DataLoader(test_dataset, batch_size=32, pin_memory=True)\n",
        "\n",
        "# # =========================================================\n",
        "# # 5. CNN + BiLSTM Model\n",
        "# # =========================================================\n",
        "# class CNN_BiLSTM(nn.Module):\n",
        "#     def __init__(self, vocab_size, embed_dim, cnn_channels, lstm_hidden_dim, num_labels, pad_idx=0):\n",
        "#         super(CNN_BiLSTM, self).__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "\n",
        "#         # CNN layers\n",
        "#         self.cnn = nn.Sequential(\n",
        "#             nn.Conv1d(embed_dim, cnn_channels, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv1d(cnn_channels, cnn_channels, kernel_size=3, padding=1),\n",
        "#             nn.ReLU()\n",
        "#         )\n",
        "\n",
        "#         # BiLSTM\n",
        "#         self.bilstm = nn.LSTM(\n",
        "#             input_size=cnn_channels,\n",
        "#             hidden_size=lstm_hidden_dim,\n",
        "#             num_layers=1,\n",
        "#             bidirectional=True,\n",
        "#             batch_first=True\n",
        "#         )\n",
        "\n",
        "#         self.dropout = nn.Dropout(0.3)\n",
        "#         self.classifier = nn.Linear(lstm_hidden_dim*2, num_labels)\n",
        "\n",
        "#     def forward(self, input_ids, labels=None):\n",
        "#         x = self.embedding(input_ids)        # (batch, seq_len, embed_dim)\n",
        "#         x = x.transpose(1, 2)                # (batch, embed_dim, seq_len) for CNN\n",
        "#         x = self.cnn(x)\n",
        "#         x = x.transpose(1, 2)                # (batch, seq_len, cnn_channels)\n",
        "#         x, _ = self.bilstm(x)                # (batch, seq_len, lstm_hidden*2)\n",
        "#         x = self.dropout(x)\n",
        "#         logits = self.classifier(x)\n",
        "#         logits = logits[:, :max_len, :]      # trim to max_len\n",
        "\n",
        "#         loss = None\n",
        "#         if labels is not None:\n",
        "#             loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "#             loss = loss_fct(logits.reshape(-1, logits.size(-1)), labels.reshape(-1))\n",
        "#         return (loss, logits) if loss is not None else logits\n",
        "\n",
        "# # =========================================================\n",
        "# # 6. Training\n",
        "# # =========================================================\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# vocab_size = len(aa_vocab) + 1\n",
        "# embed_dim = 128\n",
        "# cnn_channels = 128\n",
        "# lstm_hidden_dim = 256\n",
        "# num_labels = len(label_map)\n",
        "\n",
        "# model = CNN_BiLSTM(vocab_size, embed_dim, cnn_channels, lstm_hidden_dim, num_labels).to(device)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "# epochs = 5\n",
        "\n",
        "# for epoch in range(epochs):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "#     for batch in train_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss, logits = model(input_ids, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "#     avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "#     # Validation\n",
        "#     model.eval()\n",
        "#     val_loss = 0\n",
        "#     with torch.no_grad():\n",
        "#         for batch in val_loader:\n",
        "#             input_ids = batch[\"input_ids\"].to(device)\n",
        "#             labels = batch[\"labels\"].to(device)\n",
        "#             loss, _ = model(input_ids, labels)\n",
        "#             val_loss += loss.item()\n",
        "#     avg_val_loss = val_loss / len(val_loader)\n",
        "#     print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-04T17:45:48.531663Z",
          "iopub.status.busy": "2025-09-04T17:45:48.531439Z",
          "iopub.status.idle": "2025-09-04T17:52:16.773934Z",
          "shell.execute_reply": "2025-09-04T17:52:16.773211Z"
        },
        "id": "d4c4b811",
        "papermill": {
          "duration": 388.256044,
          "end_time": "2025-09-04T17:52:16.781245",
          "exception": false,
          "start_time": "2025-09-04T17:45:48.525201",
          "status": "completed"
        },
        "tags": [],
        "outputId": "b382c1f7-49b1-4df8-d5c3-fef356df031e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_19/1856346211.py:12: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  pss_data = pd.read_csv('/kaggle/input/maindataaaaaaaaa/2022-08-03-ss.processed.csv')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 176490\n",
            "Val size: 22061\n",
            "Test size: 22062\n",
            "Epoch 1/5 | Train Loss: 0.9248 | Val Loss: 0.7553\n",
            "Epoch 2/5 | Train Loss: 0.7176 | Val Loss: 0.6677\n",
            "Epoch 3/5 | Train Loss: 0.6588 | Val Loss: 0.6359\n",
            "Epoch 4/5 | Train Loss: 0.6276 | Val Loss: 0.6160\n",
            "Epoch 5/5 | Train Loss: 0.6085 | Val Loss: 0.6042\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "7f9284f3",
      "cell_type": "code",
      "source": [
        "# model.eval()\n",
        "# all_preds, all_labels = [], []\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch in test_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "\n",
        "#         logits = model(input_ids, labels=None)  # <-- fixed\n",
        "#         preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "#         # Flatten predictions and labels, ignore padding\n",
        "#         for p_seq, l_seq in zip(preds.cpu().numpy(), labels.cpu().numpy()):\n",
        "#             for pi, li in zip(p_seq, l_seq):\n",
        "#                 if li != -100:\n",
        "#                     all_preds.append(pi)\n",
        "#                     all_labels.append(li)\n",
        "\n",
        "# from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# target_names = list(label_map.keys())\n",
        "# print(classification_report(all_labels, all_preds, target_names=target_names, digits=4))\n",
        "# q8_accuracy = accuracy_score(all_labels, all_preds)\n",
        "# print(f\"Q8 Accuracy: {q8_accuracy:.4f}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-04T17:52:16.793991Z",
          "iopub.status.busy": "2025-09-04T17:52:16.793491Z",
          "iopub.status.idle": "2025-09-04T17:52:26.252789Z",
          "shell.execute_reply": "2025-09-04T17:52:26.251899Z"
        },
        "id": "7f9284f3",
        "papermill": {
          "duration": 9.466927,
          "end_time": "2025-09-04T17:52:26.254030",
          "exception": false,
          "start_time": "2025-09-04T17:52:16.787103",
          "status": "completed"
        },
        "tags": [],
        "outputId": "3f9ff4b4-2a43-49f2-d8c8-4f8eaae26755"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.7458    0.7884    0.7665    646157\n",
            "           E     0.8298    0.8960    0.8616    563979\n",
            "           H     0.8690    0.9328    0.8997    682331\n",
            "           B     0.8344    0.2682    0.4059     26516\n",
            "           G     0.7206    0.5150    0.6007     72668\n",
            "           I     0.9231    0.1260    0.2217       381\n",
            "           T     0.6796    0.6051    0.6402    255238\n",
            "           S     0.6656    0.4606    0.5445    210849\n",
            "\n",
            "    accuracy                         0.7922   2458119\n",
            "   macro avg     0.7835    0.5740    0.6176   2458119\n",
            "weighted avg     0.7858    0.7922    0.7843   2458119\n",
            "\n",
            "Q8 Accuracy: 0.7922\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "08e2841f",
      "cell_type": "markdown",
      "source": [
        "**Attention Mechanism+CNN**"
      ],
      "metadata": {
        "id": "08e2841f",
        "papermill": {
          "duration": 0.005824,
          "end_time": "2025-09-04T17:52:26.266061",
          "exception": false,
          "start_time": "2025-09-04T17:52:26.260237",
          "status": "completed"
        },
        "tags": []
      }
    },
    {
      "id": "5eaa2db2",
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# # =========================================================\n",
        "# # 1. Load & Clean Dataset\n",
        "# # =========================================================\n",
        "# pss_data = pd.read_csv('/kaggle/input/maindataaaaaaaaa/2022-08-03-ss.processed.csv')\n",
        "# pss_data_cleaned = pss_data.dropna(subset=['sst8'])\n",
        "\n",
        "# label_map = {'C':0,'E':1,'H':2,'B':3,'G':4,'I':5,'T':6,'S':7}\n",
        "# num_labels = len(label_map)\n",
        "# pad_idx = 0\n",
        "# max_len = 128\n",
        "\n",
        "# # =========================================================\n",
        "# # 2. Train/Val/Test Split\n",
        "# # =========================================================\n",
        "# X = pss_data_cleaned['seq']\n",
        "# y = pss_data_cleaned['sst8']\n",
        "\n",
        "# train_texts, val_texts, train_labels, val_labels = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# val_texts, test_texts, val_labels, test_labels = train_test_split(val_texts, val_labels, test_size=0.5, random_state=42)\n",
        "\n",
        "# # =========================================================\n",
        "# # 3. Encoding\n",
        "# # =========================================================\n",
        "# aa_vocab = {aa: idx+1 for idx, aa in enumerate(\"ACDEFGHIKLMNPQRSTVWY\")}\n",
        "# aa_vocab[\"X\"] = len(aa_vocab)+1\n",
        "\n",
        "# def encode_sequence(seq, max_len=max_len):\n",
        "#     ids = [aa_vocab.get(aa, aa_vocab[\"X\"]) for aa in seq]\n",
        "#     return ids[:max_len] + [pad_idx]*(max_len - len(ids))\n",
        "\n",
        "# def encode_labels(labels, max_len=max_len):\n",
        "#     ids = [label_map.get(l,-100) for l in labels]\n",
        "#     return ids[:max_len] + [-100]*(max_len - len(ids))\n",
        "\n",
        "# train_enc = [encode_sequence(s) for s in train_texts]\n",
        "# val_enc   = [encode_sequence(s) for s in val_texts]\n",
        "# test_enc  = [encode_sequence(s) for s in test_texts]\n",
        "\n",
        "# train_lab = [encode_labels(l) for l in train_labels]\n",
        "# val_lab   = [encode_labels(l) for l in val_labels]\n",
        "# test_lab  = [encode_labels(l) for l in test_labels]\n",
        "\n",
        "# # =========================================================\n",
        "# # 4. Dataset\n",
        "# # =========================================================\n",
        "# class PSSDataset(Dataset):\n",
        "#     def __init__(self, encodings, labels):\n",
        "#         self.encodings = encodings\n",
        "#         self.labels = labels\n",
        "#     def __len__(self):\n",
        "#         return len(self.encodings)\n",
        "#     def __getitem__(self, idx):\n",
        "#         return {\n",
        "#             \"input_ids\": torch.tensor(self.encodings[idx], dtype=torch.long),\n",
        "#             \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "#         }\n",
        "\n",
        "# train_dataset = PSSDataset(train_enc, train_lab)\n",
        "# val_dataset   = PSSDataset(val_enc, val_lab)\n",
        "# test_dataset  = PSSDataset(test_enc, test_lab)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True)\n",
        "# val_loader   = DataLoader(val_dataset, batch_size=32, pin_memory=True)\n",
        "# test_loader  = DataLoader(test_dataset, batch_size=32, pin_memory=True)\n",
        "\n",
        "# # =========================================================\n",
        "# # 5. CNN + Attention Model\n",
        "# # =========================================================\n",
        "# class CNNAttentionModel(nn.Module):\n",
        "#     def __init__(self, vocab_size, embed_dim, cnn_channels, num_labels, num_heads=4, pad_idx=0):\n",
        "#         super().__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "#         self.cnn = nn.Conv1d(embed_dim, cnn_channels, kernel_size=3, padding=1)\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.attention = nn.MultiheadAttention(embed_dim=cnn_channels, num_heads=num_heads, batch_first=True)\n",
        "#         self.dropout = nn.Dropout(0.3)\n",
        "#         self.classifier = nn.Linear(cnn_channels, num_labels)\n",
        "\n",
        "#     def forward(self, input_ids, labels=None):\n",
        "#         x = self.embedding(input_ids)          # (batch, seq_len, embed_dim)\n",
        "#         x = x.transpose(1,2)                   # (batch, embed_dim, seq_len)\n",
        "#         x = self.relu(self.cnn(x))\n",
        "#         x = x.transpose(1,2)                   # (batch, seq_len, cnn_channels)\n",
        "\n",
        "#         attn_out, _ = self.attention(x, x, x)  # self-attention\n",
        "#         x = self.dropout(attn_out)\n",
        "#         logits = self.classifier(x)\n",
        "\n",
        "#         loss = None\n",
        "#         if labels is not None:\n",
        "#             loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "#             loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "#         return (loss, logits) if loss is not None else logits\n",
        "\n",
        "# # =========================================================\n",
        "# # 6. Training\n",
        "# # =========================================================\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# vocab_size = len(aa_vocab)+1\n",
        "# embed_dim = 128\n",
        "# cnn_channels = 128\n",
        "# num_heads = 4\n",
        "\n",
        "# model = CNNAttentionModel(vocab_size, embed_dim, cnn_channels, num_labels, num_heads).to(device)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "# epochs = 5\n",
        "\n",
        "# for epoch in range(epochs):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "#     for batch in train_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss, _ = model(input_ids, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "#     avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "#     # Validation\n",
        "#     model.eval()\n",
        "#     val_loss = 0\n",
        "#     with torch.no_grad():\n",
        "#         for batch in val_loader:\n",
        "#             input_ids = batch[\"input_ids\"].to(device)\n",
        "#             labels = batch[\"labels\"].to(device)\n",
        "#             loss, _ = model(input_ids, labels)\n",
        "#             val_loss += loss.item()\n",
        "#     avg_val_loss = val_loss / len(val_loader)\n",
        "#     print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "# # =========================================================\n",
        "# # 7. Evaluation\n",
        "# # =========================================================\n",
        "# model.eval()\n",
        "# all_preds, all_labels = [], []\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch in test_loader:\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "#         logits = model(input_ids, labels=None)\n",
        "#         preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "#         for p_seq, l_seq in zip(preds.cpu().numpy(), labels.cpu().numpy()):\n",
        "#             for pi, li in zip(p_seq, l_seq):\n",
        "#                 if li != -100:\n",
        "#                     all_preds.append(pi)\n",
        "#                     all_labels.append(li)\n",
        "\n",
        "# target_names = list(label_map.keys())\n",
        "# print(classification_report(all_labels, all_preds, target_names=target_names, digits=4))\n",
        "# q8_accuracy = accuracy_score(all_labels, all_preds)\n",
        "# print(f\"Q8 Accuracy: {q8_accuracy:.4f}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-04T17:52:26.279136Z",
          "iopub.status.busy": "2025-09-04T17:52:26.278925Z",
          "iopub.status.idle": "2025-09-04T17:54:59.008587Z",
          "shell.execute_reply": "2025-09-04T17:54:59.007641Z"
        },
        "id": "5eaa2db2",
        "papermill": {
          "duration": 152.738117,
          "end_time": "2025-09-04T17:54:59.009926",
          "exception": false,
          "start_time": "2025-09-04T17:52:26.271809",
          "status": "completed"
        },
        "tags": [],
        "outputId": "143ddae2-f040-4066-9bc1-90bbb26875f8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_19/44030190.py:12: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  pss_data = pd.read_csv('/kaggle/input/maindataaaaaaaaa/2022-08-03-ss.processed.csv')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 | Train Loss: 1.3023 | Val Loss: 1.2949\n",
            "Epoch 2/5 | Train Loss: 1.2337 | Val Loss: 1.2188\n",
            "Epoch 3/5 | Train Loss: 1.2134 | Val Loss: 1.2071\n",
            "Epoch 4/5 | Train Loss: 1.2036 | Val Loss: 1.2087\n",
            "Epoch 5/5 | Train Loss: 1.1968 | Val Loss: 1.1959\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.5250    0.5825    0.5523    646157\n",
            "           E     0.6535    0.6018    0.6266    563979\n",
            "           H     0.5484    0.8276    0.6596    682331\n",
            "           B     0.7758    0.0946    0.1687     26516\n",
            "           G     0.6405    0.1276    0.2128     72668\n",
            "           I     0.0000    0.0000    0.0000       381\n",
            "           T     0.5005    0.2740    0.3541    255238\n",
            "           S     0.5949    0.0979    0.1682    210849\n",
            "\n",
            "    accuracy                         0.5626   2458119\n",
            "   macro avg     0.5298    0.3258    0.3428   2458119\n",
            "weighted avg     0.5705    0.5626    0.5313   2458119\n",
            "\n",
            "Q8 Accuracy: 0.5626\n"
          ]
        }
      ],
      "execution_count": null
    }
  ]
}